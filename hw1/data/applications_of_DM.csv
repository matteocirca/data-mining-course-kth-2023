"title","text"
"Anomaly Detection at Multiple Scales","Anomaly Detection at Multiple Scales, or ADAMS, was a $35 million DARPA project designed to identify patterns and anomalies in very large data sets. It is under DARPA's Information Innovation office and began in 2011 and ended in August 2014The project was intended to detect and prevent insider threats such as ""a soldier in good mental health becoming homicidal or
suicidal"", an ""innocent insider becoming malicious"", or ""a government employee [who] abuses access privileges to share classified information"". Specific cases mentioned are Nidal Malik Hasan and WikiLeaks source Chelsea Manning. Commercial applications may include finance. The intended recipients of the system output are operators in the counterintelligence agencies.The Proactive Discovery of Insider Threats Using Graph Analysis and Learning was part of the ADAMS project. The Georgia Tech team includes noted high-performance computing researcher David A. Bader.Cyber Insider Threat
Einstein (US-CERT program)
Threat (computer)
Intrusion detection"
"Behavioral analytics","Behaviorism is a systematic approach to understanding the behavior of humans and animals. It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli.  Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.
Behaviorism emerged in the early 1900s as a reaction to depth psychology and other traditional forms of psychology, which often had difficulty making predictions that could be tested experimentally, but derived from earlier research in the late nineteenth century, such as when Edward Thorndike pioneered the law of effect, a procedure that involved the use of consequences to strengthen or weaken behavior.
With a 1924 publication, John B. Watson devised methodological behaviorism, which rejected introspective methods and sought to understand behavior by only measuring observable behaviors and events. It was not until the 1930s that B. F. Skinner suggested that covert behavior—including cognition and emotions—is subject to the same controlling variables as observable behavior, which became the basis for his philosophy called radical behaviorism. While Watson and Ivan Pavlov investigated how (conditioned) neutral stimuli elicit reflexes in respondent conditioning, Skinner assessed the reinforcement histories of the discriminative (antecedent) stimuli that emits behavior; the technique became known as operant conditioning.
The application of radical behaviorism—known as applied behavior analysis—is used in a variety of contexts, including, for example, applied animal behavior and organizational behavior management to treatment of mental disorders, such as autism and substance abuse. In addition, while behaviorism and cognitive schools of psychological thought do not agree theoretically, they have complemented each other in the cognitive-behavior therapies, which have demonstrated utility in treating certain pathologies, including simple phobias, PTSD, and mood disorders.The titles given to the various branches of behaviorism include:Behavioral genetics: Proposed in 1869 by Francis Galton, a relative of Charles Darwin.
Interbehaviorism: Proposed by Jacob Robert Kantor before B. F. Skinner's writings.
Methodological behaviorism: John B. Watson's behaviorism states that only public events (motor behaviors of an individual) can be objectively observed. Although it was still acknowledged that thoughts and feelings exist, they were not considered part of the science of behavior. It also laid the theoretical foundation for the early approach behavior modification in the 1970s and 1980s.
Psychological behaviorism: As proposed by Arthur W. Staats, unlike the previous behaviorisms of Skinner, Hull, and Tolman, was based upon a program of human research involving various types of human behavior. Psychological behaviorism introduces new principles of human learning. Humans learn not only by animal learning principles but also by special human learning principles. Those principles involve humans' uniquely huge learning ability. Humans learn repertoires that enable them to learn other things. Human learning is thus cumulative. No other animal demonstrates that ability, making the human species unique.
Radical behaviorism: Skinner's philosophy is an extension of Watson's form of behaviorism by theorizing that processes within the organism—particularly, private events, such as thoughts and feelings—are also part of the science of behavior, and suggests that environmental variables control these internal events just as they control observable behaviors. Although private events cannot be directly seen by others, they are later determined through the species' overt behavior. Radical behaviorism forms the core philosophy behind behavior analysis. Willard Van Orman Quine used many of radical behaviorism's ideas in his study of knowledge and language.
Teleological behaviorism: Proposed by Howard Rachlin, post-Skinnerian, purposive, close to microeconomics. Focuses on objective observation as opposed to cognitive processes.
Theoretical behaviorism: Proposed by J. E. R. Staddon,  adds a concept of internal state to allow for the effects of context. According to theoretical behaviorism, a state is a set of equivalent histories, i.e., past histories in which members of the same stimulus class produce members of the same response class (i.e., B. F. Skinner's concept of the operant). Conditioned stimuli are thus seen to control neither stimulus nor response but state. Theoretical behaviorism is a logical extension of Skinner's class-based (generic) definition of the operant.Two subtypes of theoretical behaviorism are:Hullian and post-Hullian: theoretical, group data, not dynamic, physiological
Purposive: Tolman's behavioristic anticipation of cognitive psychologyB. F. Skinner proposed radical behaviorism as the conceptual underpinning of the experimental analysis of behavior. This viewpoint differs from other approaches to behavioral research in various ways, but, most notably here, it contrasts with methodological behaviorism in accepting feelings, states of mind and introspection as behaviors also subject to scientific investigation. Like methodological behaviorism, it rejects the reflex as a model of all behavior, and it defends the science of behavior as complementary to but independent of physiology. Radical behaviorism overlaps considerably with other western philosophical positions, such as American pragmatism.Although John B. Watson mainly emphasized his position of methodological behaviorism throughout his career, Watson and Rosalie Rayner conducted the renowned Little Albert experiment (1920), a study in which Ivan Pavlov's theory to respondent conditioning was first applied to eliciting a fearful reflex of crying in a human infant, and this became the launching point for understanding covert behavior (or private events) in radical behaviorism. However, Skinner felt that aversive stimuli should only be experimented on with animals and spoke out against Watson for testing something so controversial on a human.
In 1959, Skinner observed the emotions of two pigeons by noting that they appeared angry because their feathers ruffled. The pigeons were placed together in an operant chamber, where they were aggressive as a consequence of previous reinforcement in the environment. Through stimulus control and subsequent discrimination training, whenever Skinner turned off the green light, the pigeons came to notice that the food reinforcer is discontinued following each peck and responded without aggression. Skinner concluded that humans also learn aggression and possess such emotions (as well as other private events) no differently than do nonhuman animals.As experimental behavioural psychology is related to behavioral neuroscience, we can date the first researches in the area were done in the beginning of 19th century. Later, this essentially philosophical position gained strength from the success of Skinner's early experimental work with rats and pigeons, summarized in his books The Behavior of Organisms and Schedules of Reinforcement. Of particular importance was his concept of the operant response, of which the canonical example was the rat's lever-press. In contrast with the idea of a physiological or reflex response, an operant is a class of structurally distinct but functionally equivalent responses. For example, while a rat might press a lever with its left paw or its right paw or its tail, all of these responses operate on the world in the same way and have a common consequence. Operants are often thought of as species of responses, where the individuals differ but the class coheres in its function-shared consequences with operants and reproductive success with species. This is a clear distinction between Skinner's theory and S–R theory.
Skinner's empirical work expanded on earlier research on trial-and-error learning by researchers such as Thorndike and Guthrie with both conceptual reformulations—Thorndike's notion of a stimulus-response ""association"" or ""connection"" was abandoned; and methodological ones—the use of the ""free operant"", so-called because the animal was now permitted to respond at its own rate rather than in a series of trials determined by the experimenter procedures. With this method, Skinner carried out substantial experimental work on the effects of different schedules and rates of reinforcement on the rates of operant responses made by rats and pigeons. He achieved remarkable success in training animals to perform unexpected responses, to emit large numbers of responses, and to demonstrate many empirical regularities at the purely behavioral level. This lent some credibility to his conceptual analysis. It is largely his conceptual analysis that made his work much more rigorous than his peers, a point which can be seen clearly in his seminal work Are Theories of Learning Necessary? in which he criticizes what he viewed to be theoretical weaknesses then common in the study of psychology. An important descendant of the experimental analysis of behavior is the Society for Quantitative Analysis of Behavior.As Skinner turned from experimental work to concentrate on the philosophical underpinnings of a science of behavior, his attention turned to human language with his 1957 book Verbal Behavior and other language-related publications; Verbal Behavior laid out a vocabulary and theory for functional analysis of verbal behavior, and was strongly criticized in a review by Noam Chomsky.Skinner did not respond in detail but claimed that Chomsky failed to understand his ideas, and the disagreements between the two and the theories involved have been further discussed. Innateness theory, which has been heavily critiqued, is opposed to behaviorist theory which claims that language is a set of habits that can be acquired by means of conditioning. According to some, the behaviorist account is a process which would be too slow to explain a phenomenon as complicated as language learning. What was important for a behaviorist's analysis of human behavior was not language acquisition so much as the interaction between language and overt behavior. In an essay republished in his 1969 book Contingencies of Reinforcement, Skinner took the view that humans could construct linguistic stimuli that would then acquire control over their behavior in the same way that external stimuli could. The possibility of such ""instructional control"" over behavior meant that contingencies of reinforcement would not always produce the same effects on human behavior as they reliably do in other animals. The focus of a radical behaviorist analysis of human behavior therefore shifted to an attempt to understand the interaction between instructional control and contingency control, and also to understand the behavioral processes that determine what instructions are constructed and what control they acquire over behavior. Recently, a new line of behavioral research on language was started under the name of relational frame theory.Behaviourism focuses on one particular view of learning: a change in external behaviour achieved through using reinforcement and repetition (Rote learning) to shape behavior of learners. Skinner found that behaviors could be shaped when the use of reinforcement was implemented. Desired behavior is rewarded, while the undesired behavior is not rewarded. Incorporating behaviorism into the classroom allowed educators to assist their students in excelling both academically and personally. In the field of language learning, this type of teaching was called the audio-lingual method, characterised by the whole class using choral chanting of key phrases, dialogues and immediate correction.
Within the behaviourist view of learning, the ""teacher"" is the dominant person in the classroom and takes complete control, evaluation of learning comes from the teacher who decides what is right or wrong. The learner does not have any opportunity for evaluation or reflection within the learning process, they are simply told what is right or wrong.
The conceptualization of learning using this approach could be considered ""superficial,"" as the focus is on external changes in behaviour, i.e., not interested in the internal processes of learning leading to behaviour change and has no place for the emotions involved in the process.Operant conditioning was developed by B.F. Skinner in 1937 and deals with the management of environmental contingencies to change behavior. In other words, behavior is controlled by historical consequential contingencies, particularly reinforcement—a stimulus that increases the probability of performing behaviors, and punishment—a stimulus that decreases such probability. The core tools of consequences are either positive (presenting stimuli following a response), or negative (withdrawn stimuli following a response).The following descriptions explains the concepts of four common types of consequences in operant conditioning:
Positive reinforcement: Providing a stimulus that an individual enjoys, seeks, or craves, in order to reinforce desired behaviors. For example, when a person is teaching a dog to sit, they pair the command ""sit"" with a treat. The treat is the positive reinforcement to the behavior of sitting. The key to making positive reinforcement effect is to reward the behavior immediately.
Negative reinforcement: Removing a stimulus that an individual does not desire to reinforce desired behaviors. For example, a child hates being nagged to clean his room. His mother reinforces his room cleaning by removing the undesired stimulus of nagging after he has cleaned. Another example would be putting on sunscreen before going outside. The negative effect is getting a sunburn, so by putting on sunscreen, the behavior in this case, you avoid the stimulus of getting a sunburn.
Positive punishment: Providing a stimulus that an individual does not desire to decrease undesired behaviors. An example of this would be spanking. If a child is doing something they have been warned not to do, the parent might spank them. The undesired stimulus would be the spanking, and by adding this stimulus, the goal is to have that behavior avoided. The key to this technique is that even though the title says positive, the meaning of positive here is ""to add to."" So, in order to stop the behavior, the parent adds the adverse stimulus (spanking). The biggest problem with this type of training though is that the trainee doesn't usually learn the desired behavior, rather it teaches the trainee to avoid the punisher.
Negative punishment: Removing a stimulus that an individual desires in order to decrease undesired behaviors. An example of this would be grounding a child for failing a test. Grounding in this example is taking away the child's ability to play video games. As long as it is clear that the ability to play video games was taken away because they failed a test, this is negative punishment. The key here is the connection to the behavior and the result of the behavior.Classical experiment in operant conditioning, for example, the Skinner Box, ""puzzle box"" or operant conditioning chamber to test the effects of operant conditioning principles on rats, cats and other species. From the study of Skinner box, he discovered that the rats learned very effectively if they were rewarded frequently with food. Skinner also found that he could shape the rats' behavior through the use of rewards, which could, in turn, be applied to human learning as well.
Skinner's model was based on the premise that reinforcement is used for the desired actions or responses while punishment was used to stop the responses of the undesired actions that are not. This theory proved that humans or animals will repeat any action that leads to a positive outcome, and avoiding any action that leads to a negative outcome. The experiment with the pigeons showed that a positive outcome leads to learned behavior since the pigeon learned to peck the disc in return for the reward of food.
These historical consequential contingencies subsequently lead to (antecedent) stimulus control, but in contrast to respondent conditioning where antecedent stimuli elicit reflexive behavior, operant behavior is only emitted and therefore does not force its occurrence. It includes the following controlling stimuli:
Discriminative stimulus (Sd): An antecedent stimulus that increases the chance of the organism engaging in a behavior. One example of this occurred in Skinner's laboratory. Whenever the green light (Sd) appeared, it signaled the pigeon to perform the behavior of pecking because it learned in the past that each time it pecked, food was presented (the positive reinforcing stimulus).
Stimulus delta (S-delta): An antecedent stimulus that signals the organism not to perform a behavior since it was extinguished or punished in the past. One notable instance of this occurs when a person stops their car immediately after the traffic light turns red (S-delta). However, the person could decide to drive through the red light, but subsequently receive a speeding ticket (the positive punishing stimulus), so this behavior will potentially not reoccur following the presence of the S-delta.Although operant conditioning plays the largest role in discussions of behavioral mechanisms, respondent conditioning (also called Pavlovian or classical conditioning) is also an important behavior-analytic process that needs not refer to mental or other internal processes. Pavlov's experiments with dogs provide the most familiar example of the classical conditioning procedure. In the beginning, the dog was provided meat (unconditioned stimulus, UCS, naturally elicit a response that is not controlled) to eat, resulting in increased salivation (unconditioned response, UCR, which means that a response is naturally caused by UCS). Afterward, a bell ring was presented together with food to the dog. Although bell ring was a neutral stimulus (NS, meaning that the stimulus did not have any effect), dog would start to salivate when only hearing a bell ring after a number of pairings. Eventually, the neutral stimulus (bell ring) became conditioned. Therefore, salivation was elicited as a conditioned response (the response same as the unconditioned response), pairing up with meat—the conditioned stimulus)   Although Pavlov proposed some tentative physiological processes that might be involved in classical conditioning, these have not been confirmed. The idea of classical conditioning helped behaviorist John Watson discover the key mechanism behind how humans acquire the behaviors that they do, which was to find a natural reflex that produces the response being considered.
Watson's ""Behaviourist Manifesto"" has three aspects that deserve special recognition: one is that psychology should be purely objective, with any interpretation of conscious experience being removed, thus leading to psychology as the ""science of behaviour""; the second one is that the goals of psychology should be to predict and control behaviour (as opposed to describe and explain conscious mental states); the third one is that there is no notable distinction between human and non-human behaviour. Following Darwin's theory of evolution, this would simply mean that human behaviour is just a more complex version in respect to behaviour displayed by other species.Behaviorism is a psychological movement that can be contrasted with philosophy of mind. The basic premise of behaviorism is that the study of behavior should be a natural science, such as chemistry or physics.  Initially behaviorism rejected any reference to hypothetical inner states of organisms as causes for their behavior, but B.F. Skinner's radical behaviorism reintroduced reference to inner states and also advocated for the study of thoughts and feelings as behaviors subject to the same mechanisms as external behavior.  Behaviorism takes a functional view of behavior. According to Edmund Fantino and colleagues: ""Behavior analysis has much to offer the study of phenomena normally dominated by cognitive and social psychologists. We hope that successful application of behavioral theory and methodology will not only shed light on central problems in judgment and choice but will also generate greater appreciation of the behavioral approach.""Behaviorist sentiments are not uncommon within philosophy of language and analytic philosophy. It is sometimes argued that Ludwig Wittgenstein defended a logical behaviorist position (e.g., the beetle in a box argument). In logical positivism (as held, e.g., by Rudolf Carnap and Carl Hempel),  the meaning of psychological statements are their verification conditions, which consist of performed overt behavior. W. V. O. Quine made use of a type of behaviorism, influenced by some of Skinner's ideas, in his own work on language. Quine's work in semantics differed substantially from the empiricist semantics of Carnap which he attempted to create an alternative to, couching his semantic theory in references to physical objects rather than sensations. Gilbert Ryle defended a distinct strain of philosophical behaviorism, sketched in his book The Concept of Mind.  Ryle's central claim was that instances of dualism frequently represented ""category mistakes"", and hence that they were really misunderstandings of the use of ordinary language. Daniel Dennett likewise acknowledges himself to be a type of behaviorist, though he offers extensive criticism of radical behaviorism and refutes Skinner's rejection of the value of intentional idioms and the possibility of free will.
This is Dennett's main point in ""Skinner Skinned."" Dennett argues that there is a crucial difference between explaining and explaining away… If our explanation of apparently rational behavior turns out to be extremely simple, we may want to say that the behavior was not really rational after all. But if the explanation is very complex and intricate, we may want to say not that the behavior is not rational, but that we now have a better understanding of what rationality consists in. (Compare: if we find out how a computer program solves problems in linear algebra, we don't say it's not really solving them, we just say we know how it does it. On the other hand, in cases like Weizenbaum's ELIZA program, the explanation of how the computer carries on a conversation is so simple that the right thing to say seems to be that the machine isn't really carrying on a conversation, it's just a trick.)Law of effect: Although Edward Thorndike's methodology mainly dealt with reinforcing observable behavior, it viewed cognitive antecedents as the causes of behavior, and was theoretically much more similar to the cognitive-behavior therapies than classical (methodological) or modern-day (radical) behaviorism. Nevertheless, Skinner's operant conditioning was heavily influenced by the Law of Effect's principle of reinforcement.
Trace conditioning: Akin to B.F. Skinner's radical behaviorism, it is a respondent conditioning technique based on Ivan Pavlov's concept of a ""memory trace"" in which the observer recalls the conditioned stimulus (CS), with the memory or recall being the unconditioned response (UR). There is also a time delay between the CS and unconditioned stimulus (US), causing the conditioned response (CR)—particularly the reflex—to be faded over time.Skinner's view of behavior is most often characterized as a ""molecular"" view of behavior; that is, behavior can be decomposed into atomistic parts or molecules. This view is inconsistent with Skinner's complete description of behavior as delineated in other works, including his 1981 article ""Selection by Consequences"". Skinner proposed that a complete account of behavior requires understanding of selection history at three levels: biology (the natural selection or phylogeny of the animal); behavior (the reinforcement history or ontogeny of the behavioral repertoire of the animal); and for some species, culture (the cultural practices of the social group to which the animal belongs). This whole organism then interacts with its environment. Molecular behaviorists use notions from melioration theory, negative power function discounting or additive versions of negative power function discounting.Molar behaviorists, such as Howard Rachlin, Richard Herrnstein, and William Baum, argue that behavior cannot be understood by focusing on events in the moment. That is, they argue that behavior is best understood as the ultimate product of an organism's history and that molecular behaviorists are committing a fallacy by inventing fictitious proximal causes for behavior. Molar behaviorists argue that standard molecular constructs, such as ""associative strength"", are better replaced by molar variables such as rate of reinforcement. Thus, a molar behaviorist would describe ""loving someone"" as a pattern of loving behavior over time; there is no isolated, proximal cause of loving behavior, only a history of behaviors (of which the current behavior might be an example) that can be summarized as ""love"".Skinner's radical behaviorism has been highly successful experimentally, revealing new phenomena with new methods, but Skinner's dismissal of theory limited its development. Theoretical behaviorism recognized that a historical system, an organism, has a state as well as sensitivity to stimuli and the ability to emit responses.  Indeed, Skinner himself acknowledged the possibility of what he called ""latent"" responses in humans, even though he neglected to extend this idea to rats and pigeons.  Latent responses constitute a repertoire, from which operant reinforcement can select.  Theoretical behaviorism links between the brain and the behavior that provides a real understanding of the behavior. Rather than a mental presumption of how brain-behavior relates.Cultural analysis has always been at the philosophical core of radical behaviorism from the early days (as seen in Skinner's Walden Two, Science & Human Behavior, Beyond Freedom & Dignity, and About Behaviorism).
During the 1980s, behavior analysts, most notably Sigrid Glenn, had a productive interchange with cultural anthropologist Marvin Harris (the most notable proponent of ""cultural materialism"") regarding interdisciplinary work. Very recently, behavior analysts have produced a set of basic exploratory experiments in an effort toward this end. Behaviorism is also frequently used in game development, although this application is controversial.With the fast growth of big behavioral data and applications, behavior analysis is ubiquitous. Understanding behavior from the informatics and computing perspective becomes increasingly critical for in-depth understanding of what, why and how behaviors are formed, interact, evolve, change and affect business and decision. Behavior informatics and behavior computing deeply explore behavior intelligence and behavior insights from the informatics and computing perspectives.In the second half of the 20th century, behaviorism was largely eclipsed as a result of the cognitive revolution. This shift was due to radical behaviorism being highly criticized for not examining mental processes, and this led to the development of the cognitive therapy movement.
In the mid-20th century, three main influences arose that would inspire and shape cognitive psychology as a formal school of thought:Noam Chomsky's 1959 critique of behaviorism, and empiricism more generally, initiated what would come to be known as the ""cognitive revolution"".
Developments in computer science would lead to parallels being drawn between human thought and the computational functionality of computers, opening entirely new areas of psychological thought. Allen Newell and Herbert Simon spent years developing the concept of artificial intelligence (AI) and later worked with cognitive psychologists regarding the implications of AI. The effective result was more of a framework conceptualization of mental functions with their counterparts in computers (memory, storage, retrieval, etc.)
Formal recognition of the field involved the establishment of research institutions such as George Mandler's Center for Human Information Processing in 1964. Mandler described the origins of cognitive psychology in a 2002 article in the Journal of the History of the Behavioral SciencesIn the early years of cognitive psychology, behaviorist critics held that the empiricism it pursued was incompatible with the concept of internal mental states. Cognitive neuroscience, however, continues to gather evidence of direct correlations between physiological brain activity and putative mental states, endorsing the basis for cognitive psychology.Behavior therapy is a term referring to different types of therapies that treat mental health disorders. It identifies and helps change people's unhealthy behaviors or destructive behaviors through learning theory and conditioning. Ivan Pavlov's classical conditioning, as well as counterconditioning are the basis for much of clinical behavior therapy, but also includes other techniques, including operant conditioning—or contingency management, and modeling (sometimes called observational learning). A frequently noted behavior therapy is systematic desensitization (graduated exposure therapy), which was first demonstrated by Joseph Wolpe and Arnold Lazarus.Applied behavior analysis (ABA)—also called behavioral engineering—is a scientific discipline that applies the principles of behavior analysis to change behavior. ABA derived from much earlier research in the Journal of the Experimental Analysis of Behavior, which was founded by B.F. Skinner and his colleagues at Harvard University. Nearly a decade after the study ""The psychiatric nurse as a behavioral engineer"" (1959) was published in that journal, which demonstrated how effective the token economy was in reinforcing more adaptive behavior for hospitalized patients with schizophrenia and intellectual disability, it led to researchers at the University of Kansas to start the Journal of Applied Behavior Analysis in 1968.
Although ABA and behavior modification are similar behavior-change technologies in that the learning environment is modified through respondent and operant conditioning, behavior modification did not initially address the causes of the behavior (particularly, the environmental stimuli that occurred in the past), or investigate solutions that would otherwise prevent the behavior from reoccurring. As the evolution of ABA began to unfold in the mid-1980s, functional behavior assessments (FBAs) were developed to clarify the function of that behavior, so that it is accurately determined which differential reinforcement contingencies will be most effective and less likely for aversive consequences to be administered. In addition, methodological behaviorism was the theory underpinning behavior modification since private events were not conceptualized during the 1970s and early 1980s, which contrasted from the radical behaviorism of behavior analysis. ABA—the term that replaced behavior modification—has emerged into a thriving field.The independent development of behaviour analysis outside the United States also continues to develop. In the US, the American Psychological Association (APA) features a subdivision for Behavior Analysis, titled APA Division 25: Behavior Analysis, which has been in existence since 1964, and the interests among behavior analysts today are wide-ranging, as indicated in a review of the 30 Special Interest Groups (SIGs) within the Association for Behavior Analysis International (ABAI). Such interests include everything from animal behavior and environmental conservation, to classroom instruction (such as direct instruction and precision teaching), verbal behavior, developmental disabilities and autism, clinical psychology (i.e., forensic behavior analysis), behavioral medicine (i.e., behavioral gerontology, AIDS prevention, and fitness training), and consumer behavior analysis.
The field of applied animal behavior—a sub-discipline of ABA that involves training animals—is regulated by the Animal Behavior Society, and those who practice this technique are called applied animal behaviorists. Research on applied animal behavior has been frequently conducted in the Applied Animal Behaviour Science journal since its founding in 1974.
ABA has also been particularly well-established in the area of developmental disabilities since the 1960s, but it was not until the late 1980s that individuals diagnosed with autism spectrum disorders were beginning to grow so rapidly and groundbreaking research was being published that parent advocacy groups started demanding for services throughout the 1990s, which encouraged the formation of the Behavior Analyst Certification Board, a credentialing program that certifies professionally trained behavior analysts on the national level to deliver such services. Nevertheless, the certification is applicable to all human services related to the rather broad field of behavior analysis (other than the treatment for autism), and the ABAI currently has 14 accredited MA and Ph.D. programs for comprehensive study in that field.
Early behavioral interventions (EBIs) based on ABA are empirically validated for teaching children with autism and has been proven as such for over the past five decades. Since the late 1990s and throughout the twenty-first century, early ABA interventions have also been identified as the treatment of choice by the US Surgeon General, American Academy of Pediatrics, and US National Research Council.
Discrete trial training—also called early intensive behavioral intervention—is the traditional EBI technique implemented for thirty to forty hours per week that instructs a child to sit in a chair, imitate fine and gross motor behaviors, as well as learn eye contact and speech, which are taught through shaping, modeling, and prompting, with such prompting being phased out as the child begins mastering each skill. When the child becomes more verbal from discrete trials, the table-based instructions are later discontinued, and another EBI procedure known as incidental teaching is introduced in the natural environment by having the child ask for desired items kept out of their direct access, as well as allowing the child to choose the play activities that will motivate them to engage with their facilitators before teaching the child how to interact with other children their own age.
A related term for incidental teaching, called pivotal response treatment (PRT), refers to EBI procedures that exclusively entail twenty-five hours per week of naturalistic teaching (without initially using discrete trials). Current research is showing that there is a wide array of learning styles and that is the children with receptive language delays who initially require discrete trials to acquire speech.
Organizational behavior management, which applies contingency management procedures to model and reinforce appropriate work behavior for employees in organizations, has developed a particularly strong following within ABA, as evidenced by the formation of the OBM Network and Journal of Organizational Behavior Management, which was rated the third-highest impact journal in applied psychology by ISI JOBM rating.
Modern-day clinical behavior analysis has also witnessed a massive resurgence in research, with the development of relational frame theory (RFT), which is described as an extension of verbal behavior and a ""post-Skinnerian account of language and cognition."" RFT also forms the empirical basis for acceptance and commitment therapy, a therapeutic approach to counseling often used to manage such conditions as anxiety and obesity that consists of acceptance and commitment, value-based living, cognitive defusion, counterconditioning (mindfulness), and contingency management (positive reinforcement). Another evidence-based counseling technique derived from RFT is the functional analytic psychotherapy known as behavioral activation that relies on the ACL model—awareness, courage, and love—to reinforce more positive moods for those struggling with depression.
Incentive-based contingency management (CM) is the standard of care for adults with substance-use disorders; it has also been shown to be highly effective for other addictions (i.e., obesity and gambling). Although it does not directly address the underlying causes of behavior, incentive-based CM is highly behavior analytic as it targets the function of the client's motivational behavior by relying on a preference assessment, which is an assessment procedure that allows the individual to select the preferred reinforcer (in this case, the monetary value of the voucher, or the use of other incentives, such as prizes). Another evidence-based CM intervention for substance abuse is community reinforcement approach and family training that uses FBAs and counterconditioning techniques—such as behavioral skills training and relapse prevention—to model and reinforce healthier lifestyle choices which promote self-management of abstinence from drugs, alcohol, or cigarette smoking during high-risk exposure when engaging with family members, friends, and co-workers.
While schoolwide positive behavior support consists of conducting assessments and a task analysis plan to differentially reinforce curricular supports that replace students' disruptive behavior in the classroom, pediatric feeding therapy incorporates a liquid chaser and chin feeder to shape proper eating behavior for children with feeding disorders. Habit reversal training, an approach firmly grounded in counterconditioning which uses contingency management procedures to reinforce alternative behavior, is currently the only empirically validated approach for managing tic disorders.
Some studies on exposure (desensitization) therapies—which refer to an array of interventions based on the respondent conditioning procedure known as habituation and typically infuses counterconditioning procedures, such as meditation and breathing exercises—have recently been published in behavior analytic journals since the 1990s, as most other research are conducted from a cognitive-behavior therapy framework. When based on a behavior analytic research standpoint, FBAs are implemented to precisely outline how to employ the flooding form of desensitization (also called direct exposure therapy) for those who are unsuccessful in overcoming their specific phobia through systematic desensitization (also known as graduated exposure therapy). These studies also reveal that systematic desensitization is more effective for children if used in conjunction with shaping, which is further termed contact desensitization, but this comparison has yet to be substantiated with adults.
Other widely published behavior analytic journals include Behavior Modification, The Behavior Analyst, Journal of Positive Behavior Interventions, Journal of Contextual Behavioral Science, The Analysis of Verbal Behavior, Behavior and Philosophy, Behavior and Social Issues, and The Psychological Record.Cognitive-behavior therapy (CBT) is a behavior therapy discipline that often overlaps considerably with the clinical behavior analysis subfield of ABA, but differs in that it initially incorporates cognitive restructuring and emotional regulation to alter a person's cognition and emotions.
A popularly noted counseling intervention known as dialectical behavior therapy (DBT) includes the use of a chain analysis, as well as cognitive restructuring, emotional regulation, distress tolerance, counterconditioning (mindfulness), and contingency management (positive reinforcement). DBT is quite similar to acceptance and commitment therapy, but contrasts in that it derives from a CBT framework. Although DBT is most widely researched for and empirically validated to reduce the risk of suicide in psychiatric patients with borderline personality disorder, it can often be applied effectively to other mental health conditions, such as substance abuse, as well as mood and eating disorders.
Most research on exposure therapies (also called desensitization)—ranging from eye movement desensitization and reprocessing therapy to exposure and response prevention—are conducted through a CBT framework in non-behavior analytic journals, and these enhanced exposure therapies are well-established in the research literature for treating phobic, post-traumatic stress, and other anxiety disorders (such as obsessive-compulsive disorder, or OCD).
Cognitive-based behavioral activation (BA)—the psychotherapeutic approach used for depression—is shown to be highly effective and is widely used in clinical practice. Some large randomized control trials have indicated that cognitive-based BA is as beneficial as antidepressant medications but more efficacious than traditional cognitive therapy. Other commonly used clinical treatments derived from behavioral learning principles that are often implemented through a CBT model include community reinforcement approach and family training, and habit reversal training for substance abuse and tics, respectively.Graham, George. ""Behaviorism"".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
""Behaviorism"". Internet Encyclopedia of Philosophy."
"Business analytics","Business analysis is a professional discipline of identifying business needs and determining solutions to business problems. Solutions often include a software-systems development component, but may also consist of process improvements, organizational change or strategic planning and policy development. The person who carries out this task is called a business analyst or BA.Business analysts do not work solely on developing software systems. But work across the organisation, solving business problems in consultation with business stakeholders. Whilst most of the work that business analysts do today relate to software development/solutions, this derives from the ongoing massive changes businesses all over the world are experiencing in their attempts to digitise.Although there are different role definitions, depending upon the organization, there does seem to be an area of common ground where most
business analysts work. The responsibilities appear to be:To investigate business systems, taking a holistic view of the situation. This may include examining elements of the organisation structures and staff development issues as well as current processes and IT systems.
To evaluate actions to improve the operation of a business system. Again, this may require an examination of organisational structure and staff development needs, to ensure that they are in line with any proposed process redesign and IT system development.
To document the business requirements for the IT system support using appropriate documentation standards.In line with this, the core business analyst role could be defined as an internal consultancy role that has the responsibility for investigating business situations, identifying and evaluating options for improving business systems, defining requirements and ensuring the effective use of information systems in meeting the needs of the business.Business analysis as a discipline includes requirements analysis, sometimes also called requirements engineering. It focuses on ensuring the changes made to an organisation are aligned with its strategic goals. These changes include changes to strategies, structures, policies, business rules, processes, and information systems.
Examples of business analysis include:Focuses on understanding the needs of the business as a whole, its strategic direction, and identifying initiatives that will allow a business to meet those strategic goals. It also includes:Creating and maintaining the business architecture
Conducting feasibility studies
Identifying new business opportunities
Scoping and defining new business opportunities
Preparing the business case
Conducting the initial risk assessmentInvolves planning the requirements development process, determining which requirements are the highest priority for implementation, and managing change.Describes techniques for collecting requirements from stakeholders in a project. Techniques for requirements elicitation include:Brainstorming
Document analysis
Focus group
Interface analysis
Interviews/questionnaire
Workshops
Reverse engineering
Surveys
User task analysis
Process mapping
Observation/job shadowing
Design thinking
PrototypingDescribes how to develop and specify requirements in enough detail to allow them to be successfully implemented by a project team. Analysis 
The major forms of analysis are:Architecture analysis
Business process analysis
Object-oriented analysis
Structured analysis
Data warehouse analysis, storage and databases analysis Documentation 
Requirements documentation can take several forms:Textual – for example, stories that summarize specific information
Matrix – for example, a table of requirements with priorities
Diagrams – for example, how data flows from one structure to the other
Wireframe – for example, how elements are required in a website,
Models – for example, 3-D models that describes a character in a computer gameDescribes techniques for ensuring that stakeholders have a shared understanding of the requirements and how they will be implemented.Describes how the business analyst can perform correctness of a proposed solution, how to support the implementation of a solution, and how to assess possible shortcomings in the implementation.There are a number of generic business techniques that a business analyst will use when facilitating business change.
Some of these techniques include:This is used to perform an external environmental analysis by examining the many different external factors affecting an organization.
The six attributes of PESTLE:Political (current and potential influences from political pressures)
Economic (the local, national and world economy impact)
Sociological (the ways in which a society can affect an organization)
Technological (the effect of new and emerging technology)
Legal (the effect of national and world legislation)
Environmental (the local, national and world environmental issues)This is used to perform an in-depth analysis of early stage businesses/ventures on seven important categories:
Market opportunity
Product/solution
Execution plan
Financial engine
Human capital
Potential return
Margin of safetyIt is essentially another take on PESTLE. It factors in the same elements of PESTLE and should not be considered a tool on its own except when an author/user prefers to use this acronym as opposed to PESTLE. STEER puts into consideration the following:Socio-cultural
Technological
Economic
Ecological
Regulatory factorsThis is used to perform an internal environmental analysis by defining the attributes of MOST to ensure that the project you are working on is aligned to each of the four attributes.
The four attributes of MOST are:
Mission (where the business intends to go)
Objectives (the key goals which will help achieve the mission)
Strategies (options for moving forward)
Tactics (how strategies are put into action)SWOT is used to help focus activities into areas of strength and where the greatest opportunities lie. This is used to identify the dangers that take the form of weaknesses and both internal and external threats.
The four attributes of SWOT analysis are:Strengths – What are the advantages? What is currently done well? (e.g. key area of best-performing activities of the company)
Weaknesses – What should be improved? What is there to overcome? (e.g. key area where one is performing unsatisfactorily)
Opportunities – What good opportunities face the organization? (e.g. key area where competitors are performing poorly)
Threats – What obstacles does the organization face? (e.g. key area where competitors will perform well)This is used to prompt thinking about what the business is trying to achieve. Business perspectives help the business analyst to consider the impact of any proposed solution on the people involved.
There are six elements of CATWOE:
Customers – who are the beneficiaries of the highest level business process and how does the issue affect them?
Actors – who is involved in the situation, who will be involved in implementing solutions and what will impact their success?
Transformation process – what processes or systems are affected by the issue?
Worldview – what is the big picture and what are the wider impacts of the issue?
Owner – who owns the process or situation being investigated and what role will they play in the solution?
Environmental constraints – what are the constraints and limitations that will impact the solution and its success?This is often used in a brainstorming session to generate and analyse ideas and options. It is useful to encourage specific types of thinking and can be a convenient and symbolic way to request someone to ""switch gears"". It involves restricting the group to only thinking in specific ways – giving ideas and analysis in the ""mood"" of the time. Also known as the Six Thinking Hats.White: pure facts, logical.
Green: creative.
Yellow: bright, optimistic, positive.
Black: negative, devil's advocate.
Red: emotional.
Blue: cold, control.Not all colors/moods have to be used.Five whys is used to get to the root of what is really happening in a single instance. For each answer given, a further 'why' is asked.This is used to prioritize requirements by allocating an appropriate priority, gauging it against the validity of the requirement itself and its priority against other requirements.
MoSCoW comprises:Must have – or else delivery will be a failure
Should have – otherwise will have to adopt a workaround
Could have – to increase delivery satisfaction
Won't have this time – useful to the exclude requirements from this delivery timeframeThis technique is used when analyzing the expectations of multiple parties having different views of a system in which they all have an interest in common, but have different priorities and different responsibilities.Values – constitute the objectives, beliefs and concerns of all parties participating. They may be financial, social, tangible and intangible
Policies – constraints that govern what may be done and the manner in which it may be done
Events – real-world proceedings that stimulate activity
Content – the meaningful portion of the documents, conversations, messages, etc. that are produced and used by all aspects of business activity
Trust – between users of the system and their right to access and change information within itThe SCRS approach in business analysis claims that the analysis should flow from the high-level business strategy to the solution, through the current state and the requirements. SCRS stands for:Strategy
Current state
Requirements
SolutionThe Business Analysis Canvas is a tool that enables business analysts to quickly present a high level view of the activities that will be completed as part of the business analysis work allocation. The Business Analysis Canvas is broken into several sections.Project objective
Stakeholder
Deliverable
Impact to target operating model
Communication approach
Responsibilities
Scheduling
Key datesThe Canvas has activities and questions the business analyst can ask the organization to help build out the content.Processes are modeled visually to understand the current state and the models appear in levels to understand the enablers that are influencing a particular businesses process. At the highest level of the models are end-to-end business processes that would be common to many businesses. Below that business process level would be a level of activities, sub-activities and finally tasks. The task level is the most granular and when modeled depicts a particular workflow. As business processes get documented on the workflow level, they become more heavily influenced or ""enabled"" by characteristics that impact that particular businesses. These ""workflow enablers"" are considered to be workflow design, information systems/IT, motivation and measurement, human resources and organization, policies and rules, and facilities/physical environment. This technique of process leveling and analysis assists business analysts in understanding what is really required for a particular business and where there are possibilities to re-engineer a process for greater efficiency in the future state.As the scope of business analysis is very wide, there has been a tendency for business analysts to specialize in one of the three sets of activities which constitute the scope of business analysis, the primary role for business analysts is to identify business needs, define requirements, and provide solutions to business problems these are done as being a part of following set of activities.Strategist
Organizations need to focus on strategic matters on a more or less continuous basis in the modern business world. Business analysts, serving this need, are well-versed in analyzing the strategic profile of the organization and its environment, advising senior management on suitable policies, and the effects of policy decisions.Architect
Organizations may need to introduce change to solve business problems which may have been identified by the strategic analysis, referred to above. Business analysts contribute by analyzing objectives, processes and resources, and suggesting ways by which re-design (BPR), or improvements (BPI) could be made. Particular skills of this type of analyst are ""soft skills"", such as knowledge of the business, requirements engineering, stakeholder analysis, and some ""hard skills"", such as business process modeling. Although the role requires an awareness of technology and its uses, it is not an IT-focused role.Three elements are essential to this aspect of the business analysis effort: the redesign of core business processes; the application of enabling technologies to support the new core processes; and the management of organizational change. This aspect of business analysis is also called ""business process improvement"" (BPI), or ""reengineering"".IT-systems analyst
There is the need to align IT development with the business system as a whole. A long-standing problem in business is how to get the best return from IT investments, which are generally very expensive and of critical, often strategic, importance. IT departments, aware of the problem, often create a business analyst role to better understand and define the requirements for their IT systems. Although there may be some overlap with the developer and testing roles, the focus is always on the IT part of the change process, and generally this type of business analyst gets involved only when a case for change has already been made and decided upon.In any case, the term analyst is lately considered somewhat misleading, insofar as analysts (i.e. problem investigators) also do design work (solution definers).
The key responsibility areas of a business analyst are to collate the client's software requirements, understand them, and analyze them further from a business perspective. A business analyst is required to collaborate with and assist the business and assist them.The role of business analysis can exist in a variety of structures within an organizational framework.  Because business analysts typically act as a liaison between the business and technology functions of a company, the role can be often successful either aligned to a line of business, within IT, or sometimes both.
Business alignment
When business analysts work at the business side, they are often subject matter experts for a specific line of business. These business analysts typically work solely on project work for a particular business, pulling in business analysts from other areas for cross-functional projects.  In this case, there are usually business systems analysts on the IT side to focus on more technical requirements.IT alignment
In many cases, business analysts work solely within IT and they focus on both business and systems requirements for a project, consulting with various subject matter experts (SMEs) to ensure thorough understanding.  Depending on the organizational structure, business analysts may be aligned to a specific development lab or they might be grouped together in a resource pool and allocated to various projects based on availability and expertise.  The former builds specific subject matter expertise while the latter provides the ability to acquire cross-functional knowledge.Practice management
In a large organizations, there are centers of excellence or practice management groups who define frameworks and monitor the standards throughout the process of implementing the change in order to maintain the quality of change and reduce the risk of changes to organization. Some organizations may have independent centers of excellence for individual streams such as project management, business analysis or quality assurance.A practice management team provides a framework by which all business analysts in an organization conduct their work, usually consisting of processes, procedures, templates and best practices. In addition to providing guidelines and deliverables, it also provides a forum to focus on continuous improvement of the business analysis function.Ultimately, business analysis wants to achieve the following outcomes:Create solutions
Give enough tools for robust project management
Improve efficiency and reduce waste
Provide essential documentation, such as project initiation documentsOne way to assess these goals is to measure the return on investment (ROI) for all projects. According to Forrester Research, more than $100 billion is spent annually in the U.S. on custom and internally developed software projects.  For all of these software development projects, keeping accurate data is important and business leaders are constantly asking for the return or ROI on a proposed project or at the conclusion of an active project. However, asking for the ROI without sufficient data of where value is created or destroyed may result in inaccurate projections.Project delays are costly in several ways:Project costs – for every month of delay, the project team costs and expenses continue to accumulate. When a large part of the development team has been outsourced, the costs will start to add up quickly and are very visible if contracted on a time and materials basis (T&M). Fixed price contracts with external parties limit this risk.  For internal resources, the costs of delays are not as readily apparent, unless time spent by resources is being tracked against the project, as labor costs are essentially fixed costs.
Opportunity costs – opportunity costs come in two types – lost revenue and unrealized expense reductions. Some projects are specifically undertaken with the purpose of driving new or additional revenues to the bottom line.  For every month of delay, a company foregoes a month of this new revenue stream.  The purpose of other projects is to improve efficiencies and reduce costs.  Again, each month of failure postpones the realization of these expense reductions by another month.  In the vast majority of cases, these opportunities are never captured or analyzed, resulting in misleading ROI calculations.  Of the two opportunity costs, the lost revenue is the most egregious – and the effects are greater and longer lasting.On a lot of projects (particularly larger ones) the project manager is the one responsible for ensuring that a project is completed on time. The BA's job is more to ensure that if a project is not completed on time then at least the highest priority requirements are met.Business analysts want to make sure that they define the requirements in a way that meets the business needs, for example, in IT applications the requirements need to meet end-users' needs. Essentially, they want to define the right application.  This means that they must document the right requirements through listening carefully to customer feedback,  and by delivering a complete set of clear requirements to the technical architects and coders who will write the program.  If a business analyst has limited tools or skills to help him elicit the right requirements, then the chances are fairly high that he will end up documenting requirements that will not be used or that will need to be re-written – resulting in rework as discussed below.  The time wasted to document unnecessary requirements not only impacts the business analyst, it also impacts the rest of the development cycle.  Coders need to generate application code to perform these unnecessary requirements and testers need to make sure that the wanted features actually work as documented and coded. Experts estimate that 10% to 40% of the features in new software applications are unnecessary or go unused.  Being able to reduce the amount of these extra features by even one-third can result in significant savings. An approach of minimalism or keep it simple and minimum technology supports a reduced cost number for the result and on going maintenance of the implemented solution.Efficiency can be achieved in two ways: by reducing rework and by shortening project length.
Rework is a common industry headache and it has become so common at many organizations that it is often built into project budgets and time lines. It generally refers to extra work needed in a project to fix errors due to incomplete or missing requirements and can impact the entire software development process from definition to coding and testing. The need for rework can be reduced by ensuring that the requirements gathering and definition processes are thorough and by ensuring that the business and technical members of a project are involved in these processes from an early stage.
Shortening project length presents two potential benefits. For every month that a project can be shortened, project resource costs can be diverted to other projects. This can lead to savings on the current project and lead to earlier start times of future projects (thus increasing revenue potential).An aspiring business analyst can opt for academic or professional education. 
Several leading universities in the US, NL and UK offer master's degrees with a major in either business analysis, process management or business transformation.
There are many universities offer bachelors or master's degree in business analysis, including:The University of ManchesterMaster of Science (MSc) in business analysis
Victoria University of WellingtonMaster of professional business analysis
City University of Hong KongBBA in business analysis
Radboud University NijmegenMaster of Science (MSc) in business administration – specialisation business analysis and modellingThe three most widely recognised business analysis qualifications are:International Institute of Business Analysis (IIBA) Certified Business Analysis Professional
Level 1 – Entry-level Certificate in Business Analysis (ECBA)
Level 2 –  Certification of Capability in Business Analysis (CCBA)
Level 3 – Certified Business Analysis Professional (CBAP)
Level 4 (not yet available) –  Certified Business Analysis Thought Leader (CBATL)
Project Management Institute – Professional in Business Analysis (PMI-PBA)
The British Computer Society (BCS) offers a range of certifications and business analysis qualification:
Foundation Certificate in Business Analysis
Foundation Certificate in Business Change
Foundation Certificate in Commercial Awareness
Practitioner Certificate in Benefits Management and Business Acceptance
Practitioner Certificate in Business Analysis Practice
Practitioner Certificate in Data Management Essentials
Practitioner Certificate in Modelling Business Processes
Practitioner Certificate in Requirements Engineering
International Diploma in Business AnalysisCost overrun
Data Presentation Architecture
Enterprise Life Cycle
International Institute of Business Analysis (IIBA)
Operations research
Strategic management
Real options valuation
Requirements analysis
Revenue shortfall
Spreadmart
Viability study"
"CORE (research service)","CORE (Connecting Repositories) is a service provided by the Knowledge Media Institute based at The Open University, United Kingdom. The goal of the project is to aggregate all open access content distributed across different systems, such as repositories and open access journals, enrich this content using text mining and data mining, and provide free access to it through a set of services. The CORE project also aims to promote open access to scholarly outputs. CORE works closely with digital libraries and institutional repositories.There are existing commercial academic search systems, such as Google Scholar, which provide search and access level services, but do not support programmable machine access to the content. This is seen with the use of an API or data dumps, and limits the further reuse of the open access content (e.g., text and data mining). There are three access levels to content:
access at the granularity of papers
analytical access and granularity of collections
programmable machine access to dataThe programmable machine access is the main feature that distinguishes CORE from Google Scholar and Microsoft Academic Search.The first version of CORE was created in 2011 by Petr Knoth with the aim to make it easier to access and text mine very large amounts of research publications. The value of the aggregation was first demonstrated by developing a content recommendation system for research papers, following the ideas of literature-based discovery introduced by Don R. Swanson. Since its start, CORE has received financial support from a range of funders including Jisc and the European Commission. CORE aggregates from across the world; in 2017, it was calculated that it reached documents from 102 countries in 52 languages. It has the status of the UK's national aggregator of open access content, aggregating metadata and full-text outputs from both UK publishers' databases as well as institutional and subject repositories.CORE operates as a one step search tool for UK's open access research outputs, facilitating discoverability, use and reuse. The importance of the service has been widely recognised by Jisc, which suggested that CORE should preserve the required resources to sustain its operation and explore an international sustainability model.
CORE is now one of the Repository Shared Services projects, along with Sherpa Services, IRUS-UK, Jisc Publications Router and OpenDOAR.
In 2018, CORE said it was the world's largest aggregator of open access research papers. Based on the open access fundamental principles, as they were described in the Budapest Open Access Initiative, its open access content not only must be openly available to download and read, but it must also allow its reuse, both by humans and machines. As a result, there was a need to exploit the content reuse, which could be made possible with the implementation of a technical infrastructure. The CORE project started with the goal of connecting metadata and full-text outputs offering, through content aggregation, value-added services, and by opening new opportunities in the research process.
CORE later changed the license of its datasets to ""all rights reserved"" and was overtaken by Internet Archive Scholar, which in 2022 had over 25 million full-text articles vs. less than 10 million on CORE.CORE data can be accessed through an API or downloaded as a pre-processed and semantically enriched data dump.CORE provides searchable access to a collection of over 125 million open access harvested research outputs. All outputs can be accessed and downloaded free of cost and have limited re-use restrictions. One can search the CORE content using a faceted search. CORE also provides a cross-repository content recommendation system based on full-texts. The collection of the harvested outputs is available either by looking at the latest additions or by browsing the collection at the date of harvesting.
The CORE search engine was selected by an author on Jisc in 2013 as one of the top 10 search engines for open access research, facilitating access to academic papers.The availability of data aggregated and enriched by CORE provides opportunities for the development of new analytical services for research literature. These can be used, for example, to monitor growth and trends in research, validate compliance with open access mandates and to develop new automatic metrics for evaluating research excellence.
According to the Registry of Open Access Repositories, the number of funders increased from 22 units in 2007 to 34 in 2010 and then to 67 in 2015, while the number of institutional full-text and open access mandates picked up from 137 units in 2007 to 430 in 2015.CORE offers eight applications:CORE API, provides an access point to develop applications making use of CORE's collection of Open Access content.
CORE Dataset, provides access to the data aggregated from repositories by CORE and allows their further manipulation.
CORE Recommender, can link an institutional repository with the CORE service and recommends semantically related resources.
CORE Repository Dashboard, is a tool for repository managers or research output administrators. The aim of the Repository Dashboard is to provide control over the aggregated content and help in the management and validation of the repository collections and services. It is integrated in the  Institutional Repository Usage Statistics (IRUS-UK), a Jisc-funded project that serves as a national repository usage statistics aggregation servire.
CORE Analytics Dashboard, helps institutions to understand and monitor the impact of their research.
CORE Search, enables users to search and access research papers.
CORE Publisher Connector, provides access to Gold and Hybrid Gold Open Access articles aggregated from non-standard systems of major publishers. Data is exposed via the ResourceSync protocol.
CORE SDKs, provide access to content for programs. The CORE SDK R is freely available and it is mainly community led. The aim is to maximise the productivity and data analysis, prototyping and migration.List of academic databases and search engines
BASE (search engine)
Directory of Open Access Journals
Open Access Button
Paperity"
"Daisy Intelligence","Daisy Intelligence is a Canadian Artificial Intelligence (AI) company that provides data analysis services to help retailers, mainly grocers and supermarkets, to determine optimal pricing and promotional mix. The company also helps insurance companies detect fraudulent claims. The company uses a subset of AI known as reinforcement learning.
In October 2019, the company moved from the suburban Vaughan, Ontario, to downtown Toronto, joining other AI and technology startups concentrated in the King Street East area.In 2019, the company was ranked #39 on The Globe and Mail's annual list of Canada's ""top growing companies by three-year revenue growth."""
"Data Applied","Data Applied is a software vendor headquartered in Washington. Founded by a group of former Microsoft employees, the company specializes in data mining, data visualization, and business intelligence environments.Data Applied implements a collection of visualization tools and algorithms for data analysis and data mining. The product supports several types of analytical tasks, including visual reporting, tree maps, time series forecasting, correlation analysis, outlier detection, decision trees, association rules, clustering, and self-organizing maps."
"Data mining in agriculture","Data mining in agriculture is a recent research topic, consisting of the application of data mining techniques to agriculture. Recent technologies are able to provide extensive information on agricultural-related activities, which can then be analyzed in order to find relevant information. A related, but not equivalent term is precision agriculture.Fruit defects are often recorded (for a multitude of reasons, sometimes for insurance reasons when exporting fruit overseas). It may be done manually or through computer vision (detecting surface defects when grading fruit). Spray diaries are a legal requirement in many countries and at the very least record the date of spray and the product name. It is known that spraying can have affect different fruit defects for different fruit. Fungicidal sprays are often used to prevent rots from being expressed on fruit. It is also known that some sprays can cause russeting on apples. Currently much of this knowledge comes anecdotally, however some efforts have been in regards to the use of data mining in horticulture.Wine is widely produced worldewide. A correct fermentation process of wine is crucial, as it can impact the productivity of wine-related industries as well as the quality of the wine. If the fermentation could be categorized and predicted at the early stages of the process, it could be altered in order to guarantee a regular and smooth fermentation. The process of fermentation is currently studied by means of different techniques, such as the k-means algorithm, and classification techniques based on the concept of biclustering. These methods differ from techniques where a classification of different kinds of wine is performed. See the wiki page Classification of wine for more details.A group method of data handling-type neural network (GMDH-type network) with an evolutionary method of genetic algorithm was used to predict the metabolizable energy of feather meal and poultry offal meal based on their protein, fat, and ash content. Published data samples were collected from literature and used to train a GMDH-type network model. The  novel modeling of GMDH-type network with an evolutionary method of genetic algorithm can be used to predict the metabolizable energy of poultry feed samples based on their chemical content. It is also reported that the GMDH-type network may be used to accurately estimate the poultry performance from their dietary nutrients such as dietary metabolizable energy, protein and amino acids.The detection of animal's diseases in farms can impact positively the productivity of the farm, because sick animals can cause contaminations. Moreover, the early detection of the diseases can allow the farmer to cure the animal as soon as the disease appears. Sounds issued by pigs can be analyzed for the detection of diseases. In particular, their coughs can be studied, because they indicate their sickness. A computational system is under development which is able to monitor pig sounds by microphones installed in the farm, and which is also able to discriminate among the different sounds that can be detected.Polymerase chain reaction-single strand conformation polymorphism (PCR-SSCP) method was used to determine the growth hormone (GH), leptin, calpain, and calpastatin polymorphism in Iranian Baluchi male sheep. An artificial neural network (ANN) model was developed to describe average daily gain (ADG) in lambs from input parameters of GH, leptin, calpain, and calpastatin polymorphism, birth weight, and birth type. The results revealed that the ANN-model is an appropriate tool to recognize the patterns of data to predict lamb growth in terms of ADG given specific genes polymorphism, birth weight, and birth type. The platform of PCR-SSCP approach and ANN-based model analyses may be used in molecular marker-assisted selection and breeding programs to design a scheme in enhancing the efficacy of sheep production.Before going to market, apples are checked and the ones showing some defects are removed. However, there are also invisible defects, that can spoil the apple flavor and look. An example of invisible defect is the watercore. This is an internal apple disorder that can affect the longevity of the fruit. Apples with slight or mild watercores are sweeter, but apples with moderate to severe degree of watercore cannot be stored for any length of time. Moreover, a few fruits with severe watercore could spoil a whole batch of apples. For this reason, a computational system is under study which takes X-ray photographs of the fruit while they run on conveyor belts, and which is also able to analyse (by data mining techniques) the taken pictures and estimate the probability that the fruit contains watercores.Recent studies by agriculture researchers in Pakistan (one of the top four cotton producers of the world) showed that attempts of cotton crop yield maximization through pro-pesticide state policies have led to a dangerously high pesticide use. These studies have reported a negative correlation between pesticide use and crop yield in Pakistan. Hence excessive use (or abuse) of pesticides is harming the farmers with adverse financial, environmental and social impacts. By data mining the cotton Pest Scouting data along with the meteorological recordings it was shown that how pesticide use can be optimized (reduced). Clustering of data revealed interesting patterns of farmer practices along with pesticide use dynamics and hence help identify the reasons for this pesticide abuse.To monitor cotton growth, different government departments and agencies in Pakistan have been recording pest scouting, agriculture and metrological data for decades. Coarse estimates of just the cotton pest scouting data recorded stands at around 1.5 million records, and growing. The primary agro-met data recorded has never been digitized, integrated or standardized to give a complete picture, and hence cannot support decision making, thus requiring an Agriculture Data Warehouse. Creating a novel Pilot Agriculture Extension Data Warehouse followed by analysis through querying and  data mining some interesting discoveries were made, such as pesticides sprayed at the wrong time, wrong pesticides used for the right reasons and temporal relationship between pesticide usage and day of the week.A platform of artificial neural network-based models with sensitivity analysis and optimization algorithms was used successfully to integrate published data on the responses of broiler chickens to threonine. Analyses of the artificial neural network models for weight gain and feed efficiency from a compiled data set suggested that the dietary protein concentration was more important than the threonine concentration. The results revealed that a diet containing 18.69% protein and 0.73% threonine may lead to producing optimal weight gain, whereas the optimal feed efficiency may be achieved with a diet containing 18.71% protein and 0.75% threonine.There are a few precision agriculture journals, such as Springer's Precision Agriculture or Elsevier's Computers and Electronics in Agriculture, but those are not exclusively devoted to data mining in agriculture."
"Data thinking","Data thinking is a buzzword for the generic ""mental pattern"" observed during the processes of picking a subject to start with, identifying its parts or components, organizing and describing them in an informative fashion that is relevant to what motivated and initiated the whole process.
In the context of new product development and innovation data thinking can be described as follows: Data thinking is a framework to explore, design, develop and validate data driven solutions and user, data and future focused businesses. Data thinking combines data science with design thinking and therefore, the focus of this approach does not lie only on data analytics technologies and data collection but also on the design of use centered solutions with high business potential. The term was created by Mario Faria and Rogerio Panigassi in 2013 when they were writing a book about data science, data analytics, data management and how data practitioners were able to achieve their goals.Even though no standardized process for data thinking yet exists, the major phases of the process are similar in many publications and could be summarized as follows:During this phase the broader context of digital strategy is analyzed. Before starting with a concrete data project, it is essential to understand how the new data and AI driven technologies are affecting the business landscape and the implications this has on the future of an organization. Trend analysis / technology forecasting and scenario planning / analysis as well as internal data capability assessments are the major techniques that are typically applied at this stage. The result of the earlier stage is a definition of the focus areas which are either the most promising or are at the highest risks for or due to data driven transformation. At the ideation/exploration phase the concrete use cases are defined for the selected focus areas. For the successful ideation it is important to combine information about organizational (business) goals, internal/external use needs, data and infrastructure needs as well as domain knowledge about latest data driven technologies and trends.  Design thinking principles in the context of data thinking can be interpreted as follows: when developing data driven ideas, it is crucial to consider the intersection of technical feasibility, business impact and data availability. Typical instruments of design thinking (e.g. user research, personas, customer journey) are broadly applied on this stage. But not only user, customer and strategic needs of an organization must be considered here. Data needs and data availability analysis as well as research on the AI technologies suitable for the data based solution are essential parts of the successful development process. To scope data and the technological basement of the solution, practices from cross industry standard process for data mining (CRISP-DM)  are typically utilized on this stage. During the previous stages the major concept of the data solution was developed. At the current step, the proof of concept is conducted to check its feasibility. This stage also exploits the prototype framework of design thinking and includes test, evaluation, iteration, and refinement. Prototyping design thinking principles are also combined during this phase with process models that are applied in data science projects (e.g. CRISP-DM).Not only solution feasibility, but also its profitability is proofed during the data thinking process. Cost benefits analysis and business case calculation are commonly applied during this step.If the developed solution proves its feasibility and profitability during this phase, it will be implemented and operationalized. "
"Document processing","Document processing is a field of research and a set of production processes aimed at making an analog document digital. Document processing does not simply aim to photograph or scan a document to obtain a digital image, but also to make it digitally intelligible. This includes extracting the structure of the document or the layout and then the content, which can take the form of text or images. The process can involve traditional computer vision algorithms, convolutional neural networks or manual labor. The problems addressed are related to semantic segmentation, object detection, optical character recognition (OCR), handwritten text recognition (HTR) and, more broadly, transcription, whether automatic or not. The term can also include the phase of digitizing the document using a scanner and the phase of interpreting the document, for example using natural language processing (NLP) or image classification technologies. It is applied in many industrial and scientific fields for the optimization of administrative processes, mail processing and the digitization of analog archives and historical documents.Document processing was initially as is still to some extent a kind of production line work dealing with the treatment of documents, such as letters and parcels, in an aim of sorting, extracting or massively extracting data. This work could be performed in-house or through business process outsourcing. Document processing can indeed involve some kind of externalized manual labor, such as mechanical Turk.
As an example of manual document processing, as relatively recent as 2007, document processing for ""millions of visa and citizenship applications"" was about use of ""approximately 1,000 contract workers"" working to ""manage mail room and data entry.""
While document processing involved data entry via keyboard well before use of a computer mouse or a computer scanner, a 1990 article in The New York Times regarding what it called the ""paperless office"" stated that ""document processing begins with the scanner"". In this context, a former Xerox vice-president, Paul Strassman, expressed a critical opinion, saying that computers add rather than reduce the volume of paper in an office. It was said that the engineering and maintenance documents for an airplane weigh ""more than the airplane itself"".As the state of the art advanced, document processing transitioned to handling ""document components ... as database entities.""A technology called automatic document processing or sometimes intelligent document processing (ID) emerged as a specific form of Intelligent Process Automation (IPA), combining artificial intelligence such as Machine Learning (ML), Natural Language Processing (NAP) or Intelligent Character Recognition (ICE) to extract data from several types documents.Automatic document processing applies to a whole range of documents, whether structured or not. For instance, in the world of business and finance, technologies may be used to process paper-based invoices, forms, purchase orders, contracts, and currency bills. Financial institutions use intelligent document processing to process high volumes of forms such as regulatory forms or loan documents. ID uses AI to extract and classify data from documents, replacing manual data entry.In medicine, document processing methods have been developed to facilitate patient follow-up and streamline administrative procedures, in particular by digitizing medical or laboratory analysis reports. The goal is also to standardize medical databases. Algorithms are also directly used to assist the physicians in medical diagnosis, e.g. by analyzing magnetic resonance images, or microscopic images.Document processing is also widely used in the humanities and digital humanities, in order to extract historical big data from archives or heritage collections. Specific approaches were developed for various sources, including textual documents, such as newspaper archives, but also images, or maps.If, from the 1980s onward, traditional computer vision algorithms were widely used to solve document processing problems, these have been gradually replaced by neural network technologies in the 2010s. However, traditional computer vision technologies are still used, sometimes in conjunction with neural networks, in some sectors.
Many technologies support the development of document processing, in particular optical character recognition (OCR), and handwritten text recognition (HTR), which allow the text to be transcribed automatically. Text segments as such are identified using instance or object detection algorithms, which can sometimes also be used to detect the structure of the document. The resolution of the latter problem sometimes also uses semantic segmentation algorithms.
These technologies often form the core of document processing. However, other algorithms may intervene before or after these processes. Indeed, document digitization technologies are also involved, whether in the form of classical or three-dimensional scanning. The digitization of 3D documents can in particular resort to derivatives of photogrammetry. Sometimes, specific 2D scanners must also be developed to adapt to the size of the documents or for reasons of scanning ergonomics. The document processing also depends on the digital encoding of the documents in a suitable file format. Furthermore, the processing of heterogeneous databases can rely on image classification technologies.
At the other end of the chain are various image completion, extrapolation or data cleanup algorithms. For textual documents, the interpretation can use natural language processing (NLP) technologies.Document automation
Document modelling
Data Processing
Document Imaging
Duplex scanning
Text mining
Workflow"
"Equifax Workforce Solutions","Equifax Workforce Solutions, formerly known as TALX (pronounced ""talks""), is a wholly owned subsidiary of Equifax. It is based in St. Louis, Missouri. The company was originally founded in 1972 under the name Interface Technology Inc. The company maintains a database named ""The Work Number"" that holds and maintains employment and payroll information on 54 million American people. As of 2015, the company was the largest source of employment information in the United States, and collects information from over 7,000 employers.Based out of St. Louis, TALX was founded in 1972 as Interface Technology Inc. by several individuals including H. Richard ""Rick"" Grodsky, Professor of Electrical Engineering at Washington University. Interface Technology provided interactive voice response systems. Bill Canfield joined the company in 1986 as President and CEO and added the title of Chairman in 1988. TALX went public listing on the NASDAQ in an IPO in 1996 and offered 2,000,000 shares at $9 per share for a total offer amount of $18,000,000. TALX Corp. At the time of the IPO, TALX designed and implemented interactive communication solutions using computer telephony to integrate technologies such as interactive voice response, fax, email, Internet and corporate intranet. TALX's interactive communication solutions enabled an organization's employees, customers, vendors and business partners to access, input and update information stored in databases without human assistance. TALX also provided a branded employment and income verification service, The Work Number for Everyone, that a provided automated access to employment and salary records of large employers for purposes of loan and other credit approvals.
In the early 2000s, the employment and income verification service, The Work Number, as it later became known, became the revenue and profit growth engine for the company. In March 2002, TALX Corporation acquired two large human resource outsourcing companies that specialized in unemployment cost management and related human resource applications, The Frick Company, headquartered in St. Louis, Missouri, and Gates McDonald, a subsidiary of Nationwide Mutual Insurance Company, headquartered in Columbus, Ohio. TALX sold its e-Choice Benefits Enrollment Services business to Workscape in April 2003 to focus on its core business of payroll-centric services.Between 2002 and 2005, acquired Johnson & Associates LLC, TBT Enterprises Inc., UI Advantage Inc., Jon-Jay Associates Inc., Employers Unity Inc. and parts of Sheakley-Uniservice Inc. TALX also added or created a number of other payroll-centric Human Resource related employer services including W-2 Management, I-9 Management, Tax Credit and Incentive Management, and Online Paperless Pay.In 2007, TALX was acquired by Equifax, one of the big three credit reporting agencies, in a transaction valued at $1.4 billion.  As of 2010, integration was completed and TALX now officially operates as a division of Equifax.In October 2012, Equifax changed the name of the TALX business unit to Equifax Workforce Solutions.The Securities and Exchange Commission (SEC) conducted an investigation into TALX's August 2001 secondary offering of common stock and 2001 financial results. In August 2004, TALX reached an agreement with the SEC to end the ongoing SEC investigation, agreeing that the company would pay a fine of $2.5 million. Separately, William W. Canfield, the company's president and chief executive officer, reached an agreement in principle with the SEC staff to settle its ongoing investigation against him in a related matter.In March 2005, TALX announced that the SEC had accepted the previously announced offer of settlement submitted by TALX to resolve the SEC's investigation into its accounting for certain items. All financial statements in question have been previously restated to address the issues raised by the SEC. TALX agreed, without admitting or denying any liability, not to violate certain provisions of the Federal securities laws in the future. TALX also agreed to pay one dollar in disgorgement and $2.5 million in civil penalties. These amounts were paid into escrow by TALX in December 2004 and had been previously reflected in the company's financial statements. The SEC also accepted the offer of settlement submitted by William W. Canfield, TALX's president and chief executive officer, to resolve charges stemming from the same accounting issues. Canfield agreed, without admitting or denying any liability, not to violate certain provisions of the Federal securities laws in the future. He also agreed to pay $859,999 in disgorgement and $100,000 in civil penalties.The SEC filed fraud charges March 2005 against TALX Corp.'s former chief financial officer. The SEC alleged that Craig N. Cohen, who resigned in January 2004, violated antifraud and other federal securities laws by causing TALX to meet its 2001 financial target through fraudulent accounting practices. As a result, TALX overstated its 2001 income by about $2.1 million, or 65 percent, which inflated its stock price. Cohen then sold TALX shares. He is also accused of making misleading statements to auditors. The SEC sought a permanent injunction against Cohen, an officer and director bar and civil penalties. Cohen had served as chief financial officer from January 1994 to May 2003, and was vice president of application services and software from May 1999 to May 2003. He resigned at the same time TALX said it would restate its earnings for the fiscal years 1999 to 2003 to correct errors in the way it accounted for revenue. April 2007 the US District Court dismissed six of the seven counts against Cohen. The Court found Cohen guilty of the allegation of insufficient internal controls. The Court stated that there was evidence that Cohen knew he was falsely recording two projects as bill-and-hold transactions. The Court imposed a civil penalty against Cohen in the amount of $5,000.In June 2006 TALX announced that it was voluntarily responding to an initial inquiry by the Federal Trade Commission (FTC) to assess whether TALX acquisitions in the unemployment compensation and Work Number businesses had significantly reduced competition. TALX believed it has complied with applicable regulatory filing requirements and intends to cooperate with the inquiry.In April 2010, The New York Times published an article about TALX. In short, TALX was accused of contesting unemployment benefits claims regardless of their merit in an effort to reduce the funds their clients — the employers — would have to pay to state unemployment insurance pools. The article pointed out that some unemployed persons were denied benefits as a result of TALX's actions.The Work Number"
"Game analytics","Game analytics is the form of behavioral analytics that deals with video games. Game analytics involve using quantitative measures, metrics, and tools that can be used to track events that occur over the course of a game, with the goal of capturing such data for statistical analysis. A simple example would be programming a video game to record the number of time each players die in each  level and send the data back to the developer, so that developer will know whether some of the levels may be too difficult (i.e., with an excessively high number of player dying) and thus need redesign. The aim of using a game analytics platform is to generate insights to inform developers with regards to player behaviors and business decisions.Behavioral analytics"
"Inference attack","An Inference Attack is a data mining technique performed by analyzing data in order to illegitimately gain knowledge about a subject or database. A subject's sensitive information can be considered as leaked if an adversary can infer its real value with a high confidence. This is an example of breached information security. An Inference attack occurs when a user is able to infer from trivial information more robust information about a database without directly accessing it. The object of Inference attacks is to piece together information at one security level to determine a fact that should be protected at a higher security level.While inference attacks were originally discovered as a threat in statistical databases, today they also pose a major privacy threat in the domain of mobile and IoT sensor data. Data from accelerometers, which can be accessed by third-party apps without user permission in many mobile devices, has been used to infer rich information about users based on the recorded motion patterns (e.g., driving behavior, level of intoxication, age, gender, touchscreen inputs, geographic location).
Highly sensitive inferences can also be derived, for example, from eye tracking data, smart meter data and voice recordings (e.g., smart speaker voice commands)."
"Path analysis (computing)","Path analysis, is the analysis of a path, which is a portrayal of a chain of consecutive events that a given user or cohort performs during a set period of time while using a website, online game, or eCommerce platform. As a subset of behavioral analytics, path analysis is a way to understand user behavior in order to gain actionable insights into the data. Path analysis provides a visual portrayal of every event a user or cohort performs as part of a path during a set period of time.
While it is possible to track a user's path through the site, and even show that path as a visual representation, the real question is how to gain these actionable insights. If path analysis simply outputs a ""pretty""  graph, while it may look nice, it does not provide anything concrete to act upon.In order to get the most out of path analysis the first step would be to determine what needs to be analyzed and what are the goals of the analysis. A company might be trying to figure out why their site is running slow, are certain types of users interested in certain pages or products, or if their user interface is set up in a logical way.
Now that the goal has been set there are a few ways of performing the analysis. If a large percentage of a certain cohort, people between the ages of 18–25, logs into an online game, creates a profile and then spends the next 10 minutes wandering around the menu page, then it may be that the user interface is not logical. By seeing this group of users following the path that they did a developer will be able to analyze the data and realize that after creating a profile, the “play game” button does not appear. Thus, path analysis was able to provide actionable data for the company to act on and fix an error.
In eCommerce, path analysis can help customize a shopping experience to each user. By looking at what products other customers in a certain cohort looked at before buying one, a company can suggest “items you may also like” to the next customer and increase the chances of them making a purchase. Also, path analysis can help solve performance issues on a platform. For example, a company looks at a path and realizes that their site freezes up after a certain combinations of events. By analyzing the path and the progression of events that led to the error, the company can pinpoint the error and fix it.Historically path analysis fell under the broad category of website analytics, and related only to the analysis of paths through websites. Path analysis in website analytics is a process of determining a sequence of pages visited in a visitor session prior to some desired event, such as the visitor purchasing an item or requesting a newsletter.  The precise order of pages visited may or may not be important and may or may not be specified.  In practice, this analysis is done in aggregate, ranking the paths (sequences of pages) visited prior to the desired event, by descending frequency of use.  The idea is to determine what features of the website encourage the desired result.  ""Fallout analysis,"" a subset of path analysis, looks at ""black holes"" on the site, or paths that lead to a dead end most frequently, paths or features that confuse or lose potential customers.
With the advent of big data along with web-based applications, online games, and eCommerce platforms, path analysis has come to include much more than just web path analysis. Understanding how users move through an app, game, or other web platform are all part of modern-day path analysis.In the real world when you visit a shop the shelves and products are not placed in a random order. The shop owner carefully analyzes the visitors and path they walk through the shop, especially when they are selecting or buying products. Next the shop owner will reorder the shelves and products to optimize sales by putting everything in the most logical order for the visitors. In a supermarket this will typically result in the wine shelf next to a variety of cookies, chips, nuts, etc. Simply because people drink wine and eat nuts with it.
In most web sites there is a same logic that can be applied. Visitors who have questions about a product will go to the product information or support section of a web site. From there they make a logical step to the frequently asked questions page if they have a specific question. A web site owner also wants to analyze visitor behavior. For example, if a web site offers products for sale, the owner wants to convert as many visitors to a completed purchase. If there is a sign-up form with multiple pages, web site owners want to guide visitors to the final sign-up page.
Path analysis answers typical questions like: Where do most visitors go after they enter my home page?Is there a strong visitor relation between product A and product B on my web site?. 
Questions that can't be answered by page hits and unique visitors statistics.Google Analytics provides a path function with funnels and goals. A predetermined path of web site pages is specified and every visitor walking the path is a goal. This approach is very helpful when analyzing how many visitors reach a certain destination page, called an end point analysis.The paths visitors walk in a web site can lead to an endless number of unique paths. As a result, there is no point in analyzing each path, but to look for the strongest paths. These strongest paths are typically shown in a graphical map or in text like: Page A --> Page B --> Page D --> Exit.Funnel analysis
Cohort analysis
website analytics
Big data
Data mining
Analytics
Business Intelligence
Test and Learn
Business Process Discovery
Statistics
Customer dynamics
Behavioral analytics"
"Automatic number-plate recognition in the United Kingdom","Automatic number-plate recognition (ANPR; see also other names below) is a technology that uses optical character recognition on images to read vehicle registration plates to create vehicle location data. It can use existing closed-circuit television, road-rule enforcement cameras, or cameras specifically designed for the task. ANPR is used by police forces around the world for law enforcement purposes, including to check if a vehicle is registered or licensed. It is also used for electronic toll collection on pay-per-use roads and as a method of cataloguing the movements of traffic, for example by highways agencies.
Automatic number-plate recognition can be used to store the images captured by the cameras as well as the text from the license plate, with some configurable to store a photograph of the driver. Systems commonly use infrared lighting to allow the camera to take the picture at any time of day or night.  ANPR technology must take into account plate variations from place to place.
Privacy issues have caused concerns about ANPR, such as government tracking citizens' movements, misidentification, high error rates, and increased government spending. Critics have described it as a form of mass surveillance.ANPR is sometimes known by various other terms:Automatic (or automated) license-plate recognition (ALPR)
Automatic (or automated) license-plate reader (ALPR)
Automatic vehicle identification (AVI)
Automatisk nummerpladegenkendelse (ANPG)
Car-plate recognition (CPR)
License-plate recognition (LPR)
Lecture automatique de plaques d'immatriculation (LAPI)
Mobile license-plate reader (MLPR)
Vehicle license-plate recognition (VLPR)
Vehicle recognition identification (VRI)ANPR was invented in 1976 at the Police Scientific Development Branch in Britain. Prototype systems were working by 1979, and contracts were awarded to produce industrial systems, first at EMI Electronics, and then at Computer Recognition Systems (CRS, now part of Jenoptik) in Wokingham, UK. Early trial systems were deployed on the A1 road and at the Dartford Tunnel. The first arrest through detection of a stolen car was made in 1981. However, ANPR did not become widely used until new developments in cheaper and easier to use software were pioneered during the 1990s. The collection of ANPR data for future use (i.e., in solving then-unidentified crimes) was documented in the early 2000s.  The first documented case of ANPR being used to help solve a murder occurred in November 2005, in Bradford, UK, where ANPR played a vital role in locating and subsequently convicting killers of Sharon Beshenivsky.The software aspect of the system runs on standard home computer hardware and can be linked to other applications or databases. It first uses a series of image manipulation techniques to detect, normalize and enhance the image of the number plate, and then optical character recognition (OCR) to extract the alphanumerics of the license plate. ANPR systems are generally deployed in one of two basic approaches: one allows for the entire process to be performed at the lane location in real-time, and the other transmits all the images from many lanes to a remote computer location and performs the OCR process there at some later point in time. When done at the lane site, the information captured of the plate alphanumeric, date-time, lane identification, and any other information required is completed in approximately 250 milliseconds. This information can easily be transmitted to a remote computer for further processing if necessary, or stored at the lane for later retrieval. In the other arrangement, there are typically large numbers of PCs used in a server farm to handle high workloads, such as those found in the London congestion charge project. Often in such systems, there is a requirement to forward images to the remote server, and this can require larger bandwidth transmission media.ANPR uses optical character recognition (OCR) on images taken by cameras. When Dutch vehicle registration plates switched to a different style in 2002, one of the changes made was to the font, introducing small gaps in some letters (such as P and R) to make them more distinct and therefore more legible to such systems.  Some license plate arrangements use variations in font sizes and positioning—ANPR systems must be able to cope with such differences to be truly effective.  More complicated systems can cope with international variants, though many programs are individually tailored to each country.
The cameras used can be existing road-rule enforcement or closed-circuit television cameras, as well as mobile units, which are usually attached to vehicles. Some systems use infrared cameras to take a clearer image of the plates. In mobile systems During the 1990s, significant advances in technology took automatic number-plate recognition (ANPR) systems from limited expensive, hard to set up, fixed based applications to simple ""point and shoot"" mobile ones. This was made possible by the creation of software that ran on cheaper PC based, non-specialist hardware that also no longer needed to be given the pre-defined angles, direction, size and speed in which the plates would be passing the camera's field of view. Further scaled-down components at lower price points led to a record number of deployments by law enforcement agencies globally. Smaller cameras with the ability to read license plates at higher speeds, along with smaller, more durable processors that fit in the trunks of police vehicles, allowed law enforcement officers to patrol daily with the benefit of license plate reading in real time, when they can interdict immediately.
Despite their effectiveness, there are noteworthy challenges related with mobile ANPRs. One of the biggest is that the processor and the cameras must work fast enough to accommodate relative speeds of more than 100 mph (160 km/h), a likely scenario in the case of oncoming traffic. This equipment must also be very efficient since the power source is the vehicle electrical system, and equipment must have minimal space requirements.
Relative speed is only one issue that affects the camera's ability to read a license plate. Algorithms must be able to compensate for all the variables that can affect the ANPR's ability to produce an accurate read, such as time of day, weather and angles between the cameras and the license plates. A system's illumination wavelengths can also have a direct impact on the resolution and accuracy of a read in these conditions.
Installing ANPR cameras on law enforcement in the vehicles requires careful consideration of the juxtaposition of the cameras to the license plates they are to read. Using the right number of cameras and positioning them accurately for optimal results can prove challenging, given the various missions and environments at hand. Highway patrol requires forward-looking cameras that span multiple lanes and are able to read license plates at high speeds. City patrol needs shorter range, lower focal length cameras for capturing plates on parked cars. Parking lots with perpendicularly parked cars often require a specialized camera with a very short focal length. Most technically advanced systems are flexible and can be configured with a number of cameras ranging from one to four which can easily be repositioned as needed. States with rear-only license plates have an additional challenge since a forward-looking camera is ineffective with oncoming traffic. In this case one camera may be turned backwards.There are seven primary algorithms that the software requires for identifying a license plate:Plate localization – responsible for finding and isolating the plate on the picture
Plate orientation and sizing – compensates for the skew of the plate and adjusts the dimensions to the required size
Normalization – adjusts the brightness and contrast of the image
Character segmentation – finds the individual characters on the plates
Optical character recognition
Syntactical/Geometrical analysis – check characters and positions against country-specific rules
The averaging of the recognised value over multiple fields/images to produce a more reliable or confident result, especially given that any single image may contain a reflected light flare, be partially obscured, or possess other obfuscating effects.The complexity of each of these subsections of the program determines the accuracy of the system.  During the third phase (normalization), some systems use edge detection techniques to increase the picture difference between the letters and the plate backing. A median filter may also be used to reduce the visual noise on the image. Difficulties There are a number of possible difficulties that the software must be able to cope with. These include:Poor file resolution, usually because the plate is too far away but sometimes resulting from the use of a low-quality camera
Blurry images, particularly motion blur
Poor lighting and low contrast due to overexposure, reflection or shadows
An object obscuring (part of) the plate, quite often a tow bar, or dirt on the plate
Read license plates that are different at the front and the back because of towed trailers, campers, etc.
Vehicle lane change in the camera's angle of view during license plate reading
A different font, popular for vanity plates (some countries do not allow such plates, eliminating the problem)
Circumvention techniques
Lack of coordination between countries or states. Two cars from different countries or states can have the same number but different design of the plate.While some of these problems can be corrected within the software, it is primarily left to the hardware side of the system to work out solutions to these difficulties. Increasing the height of the camera may avoid problems with objects (such as other vehicles) obscuring the plate but introduces and increases other problems, such as adjusting for the increased skew of the plate.
On some cars, tow bars may obscure one or two characters of the license plate. Bikes on bike racks can also obscure the number plate, though in some countries and jurisdictions, such as Victoria, Australia, ""bike plates"" are supposed to be fitted. Some small-scale systems allow for some errors in the license plate. When used for giving specific vehicles access to a barricaded area, the decision may be made to have an acceptable error rate of one character. This is because the likelihood of an unauthorized car having such a similar license plate is seen as quite small. However, this level of inaccuracy would not be acceptable in most applications of an ANPR system.At the front end of any ANPR system is the imaging hardware which captures the image of the license plates. The initial image capture forms a critically important part of the ANPR system which, in accordance to the garbage in, garbage out principle of computing, will often determine the overall performance.
License plate capture is typically performed by specialized cameras designed specifically for the task, although new software techniques are being implemented that support any IP-based surveillance camera and increase the utility of ANPR for perimeter security applications. Factors which pose difficulty for license plate imaging cameras include the speed of the vehicles being recorded, varying level of ambient light, headlight glare and harsh environmental conditions. Most dedicated license plate capture cameras will incorporate infrared illumination in order to solve the problems of lighting and plate reflectivity.Many countries now use license plates that are retroreflective. This returns the light back to the source and thus improves the contrast of the image. In some countries, the characters on the plate are not reflective, giving a high level of contrast with the reflective background in any lighting conditions. A camera that makes use of active infrared imaging (with a normal colour filter over the lens and an infrared illuminator next to it) benefits greatly from this as the infrared waves are reflected back from the plate. This is only possible on dedicated ANPR cameras, however, and so cameras used for other purposes must rely more heavily on the software capabilities. Further, when a full-colour image is required as well as use of the ANPR-retrieved details, it is necessary to have one infrared-enabled camera and one normal (colour) camera working together.
To avoid blurring it is ideal to have the shutter speed of a dedicated camera set to 1/1000 of a second. It is also important that the camera use a global shutter, as opposed to rolling shutter, to assure that the taken images are distortion-free. Because the car is moving, slower shutter speeds could result in an image which is too blurred to read using the OCR software, especially if the camera is much higher up than the vehicle. In slow-moving traffic, or when the camera is at a lower level and the vehicle is at an angle approaching the camera, the shutter speed does not need to be so fast. Shutter speeds of 1/500 of a second can cope with traffic moving up to 40 mph (64 km/h) and 1/250 of a second up to 5 mph (8 km/h). License plate capture cameras can produce usable images from vehicles traveling at 120 mph (190 km/h).
To maximize the chances of effective license plate capture, installers should carefully consider the positioning of the camera relative to the target capture area. Exceeding threshold angles of incidence between camera lens and license plate will greatly reduce the probability of obtaining usable images due to distortion. Manufacturers have developed tools to help eliminate errors from the physical installation of license plate capture cameras. Australia 
Several State Police Forces, and the Department of Justice (Victoria) use both fixed and mobile ANPR systems. The New South Wales Police Force Highway Patrol were the first to trial and use a fixed ANPR camera system in Australia in 2005. In 2009 they began a roll-out of a mobile ANPR system (known officially as MANPR) with three infrared cameras fitted to its Highway Patrol fleet. The system identifies unregistered and stolen vehicles as well as disqualified or suspended drivers as well as other 'persons of interest' such as persons having outstanding warrants. Belgium 
The city of Mechelen uses an ANPR system since September 2011 to scan all cars crossing the city limits (inbound and outbound). Cars listed on 'black lists' (no insurance, stolen, etc.) generate an alarm in the dispatching room, so they can be intercepted by a patrol.
As of early 2012, 1 million cars per week are automatically checked in this way. Canada 
Federal, provincial, and municipal police services across Canada use automatic licence plate recognition software; they are also used on certain toll routes and by parking enforcement agencies. Laws governing usage of information thus obtained use of such devices are mandated through various provincial privacy acts. Denmark 
The technique is tested by the Danish police. It has been in permanent use since mid 2016. France 
180 gantries over major roads have been built throughout the country. These together with a further 250 fixed cameras is to enable a levy of an eco tax on lorries over 3.5 tonnes. The system is currently being opposed and whilst they may be collecting data on vehicles passing the cameras, no eco tax is being charged. Germany 
On 11 March 2008, the Federal Constitutional Court of Germany ruled that some areas of the laws permitting the use of automated number plate recognition systems in Germany violated the right to privacy. More specifically, the court found that the retention of any sort of information (i.e., number plate data) which was not for any pre-destined use (e.g., for use tracking suspected terrorists or for enforcement of speeding laws) was in violation of German law.
These systems were provided by Jenoptik Robot GmbH, and called TraffiCapture. Hungary In 2012 a state consortium was formed among the Hungarian Ministry of Interior, the National Police Headquarters and the Central Commission of Public Administration and Electronic Services with the aim to install and operate a unified intelligent transportation system (ITS) with nationwide coverage by the end of 2015. Within the system, 160 portable traffic enforcement and data-gathering units and 365 permanent gantry installations were brought online with ANPR, speed detection, imaging and statistical capabilities. Since all the data points are connected to a centrally located ITS, each member of the consortium is able to separately utilize its range of administrative and enforcement activities, such as remote vehicle registration and insurance verification, speed, lane and traffic light enforcement and wanted or stolen vehicle interception among others.
Several Hungarian auxiliary police units also use a system called Matrix Police in cooperation with the police. It consists of a portable computer equipped with a web camera that scans the stolen car database using automatic number-plate recognition. The system is installed on the dashboard of selected patrol vehicles (PDA-based hand-held versions also exist) and is mainly used to control the license plate of parking cars. As the Auxiliary Police do not have the authority to order moving vehicles to stop, if a stolen car is found, the formal police is informed. Saudi Arabia 
Vehicle registration plates in Saudi Arabia use white background, but several vehicle types may have a different background. There are only 17 Arabic letters used on the registration plates. A challenge for plates recognition in Saudi Arabia is the size of the digits. Some plates use both Eastern Arabic numerals and the 'Western Arabic' equivalents. A research with source code is available for APNR Arabic digits. Sweden 
The technique is tested by the Swedish Police Authority at nine different locations in Sweden. Turkey 
Several cities have tested—and some have put into service—the KGYS (Kent Guvenlik Yonetim Sistemi, City Security Administration System), i.e., capital Ankara, has debuted KGYS-  which consists of a registration plate number recognition system on the main arteries and city exits. The system has been used with two cameras per lane, one for plate recognition, one for speed detection. Now the system has been widened to network all the registration number cameras together, and enforcing average speed over preset distances. Some arteries have 70 km/h (43 mph) limit, and some 50 km/h (31 mph), and photo evidence with date-time details are posted to registration address if speed violation is detected. As of 2012, the fine for exceeding the speed limit for more than 30% is approximately US$175. Ukraine 
The project of system integration «OLLI Technology» and the Ministry of Internal Affairs of Ukraine Department of State Traffic Inspection (STI) experiments on the introduction of a modern technical complex which is capable to locate stolen cars, drivers deprived of driving licenses and other problem cars in real time. The Ukrainian complex ""Video control"" working by a principle of video fixing of the car with recognition of license plates with check under data base. United Kingdom The Home Office states the purpose of automatic number-plate recognition in the United Kingdom is to help detect, deter and disrupt criminality including tackling organised crime groups and terrorists. Vehicle movements are recorded by a network of nearly 8000 cameras capturing between 25 and 30 million ANPR ‘read’ records daily. These records are stored for up to two years in the National ANPR Data Centre, which can be accessed, analysed and used as evidence as part of investigations by UK law enforcement agencies.In 2012, the UK Parliament enacted the Protection of Freedoms Act which includes several provisions related to controlling and restricting the collection, storage, retention, and use of information about individuals. Under this Act, the Home Office published a code of practice in 2013 for the use of surveillance cameras, including ANPR, by government and law enforcement agencies. The aim of the code is to help ensure their use is ""characterised as surveillance by consent, and such consent on the part of the community must be informed consent and not assumed by a system operator. Surveillance by consent should be regarded as analogous to policing by consent."" In addition, a set of standards were introduced in 2014 for data, infrastructure, and data access and management. United States In the United States, ANPR systems are more commonly referred to as ALPR (Automatic License Plate Reader/Recognition) technology, due to differences in language (i.e., ""number plates"" are referred to as ""license plates"" in American English)
Since 2019, private companies like Flock Safety have grown rapidly, promoting stationary ALPR cameras to private individuals as well as neighbourhood associations and law enforcement. By April 2022, 1500 cities across the United States had implemented Flock cameras, despite criticism from the ACLU and other civil rights organisations and concerns about whether the system actual reduces crime.Mobile ANPR use is widespread among US law enforcement agencies at the city, county, state and federal level. According to a 2012 report by the Police Executive Research Forum, approximately 71% of all US police departments use some form of ANPR. Mobile ANPR is becoming a significant component of municipal predictive policing strategies and intelligence gathering, as well as for recovery of stolen vehicles, identification of wanted felons, and revenue collection from individuals who are delinquent on city or state taxes or fines, or monitoring for Amber Alerts. With the widespread implementation of this technology, many U.S. states now issue misdemeanor citations of up to $500 when a license plate is identified as expired or on the incorrect vehicle. Successfully recognized plates may be matched against databases including ""wanted person"", ""protection order"", missing person, gang member, known and suspected terrorist, supervised release, immigration violator, and National Sex Offender lists. In addition to the real-time processing of license plate numbers, ANPR systems in the US collect (and can indefinitely store) data from each license plate capture. Images, dates, times and GPS coordinates can be stockpiled and can help place a suspect at a scene, aid in witness identification, pattern recognition or the tracking of individuals.
The Department of Homeland Security has proposed a federal database to combine all monitoring systems, which was cancelled after privacy complaints. In 1998, a Washington, D.C. police lieutenant pleaded guilty to extortion after blackmailing the owners of vehicles parked near a gay bar. In 2015, the Los Angeles Police Department proposed sending letters to the home addresses of all vehicles that enter areas of high prostitution.Early private sector mobile ANPR applications have been for vehicle repossession and recovery, although the application of ANPR by private companies to collect information from privately owned vehicles or collected from private property (for example, driveways) has become an issue of sensitivity and public debate. Other ANPR uses include parking enforcement, and revenue collection from individuals who are delinquent on city or state taxes or fines. The technology is often featured in the reality TV show Parking Wars featured on A&E Network. In the show, tow truck drivers and booting teams use the ANPR to find delinquent vehicles with high amounts of unpaid parking fines. Laws 
Laws vary among the states regarding collection and retention of license plate information. As of 2019, 16 states have limits on how long the data may be retained, with the lowest being New Hampshire (3 minutes) and highest Colorado (3 years). The Supreme Court of Virginia ruled in 2018 that data collected from ALPRs can constitute personal information. As a result, on 1 April 2019, a Fairfax County judge issued an injunction prohibiting the Fairfax County Police Department from collecting and storing ALPR data outside of an investigation or intelligence gathering related to a criminal investigation. On October 22, 2020, the Supreme Court of Virginia overturned that decision, ruling that the data collected was not personal, identifying information.In April 2020, the Massachusetts Supreme Judicial Court found that the warrantless use of automated  license plate readers to surveil a suspected heroin distributor's bridge crossings to Cape Cod did not violate the Fourth Amendment to the United States Constitution only because of the limited time and scope of the observations.ANPR is used for speed limit enforcement in Australia, Austria, Belgium, Dubai (UAE), France, Italy, The Netherlands, Spain, South Africa, the UK, and Kuwait.This works by tracking vehicles' travel time between two fixed points, and calculating the average speed. These cameras are claimed to have an advantage over traditional speed cameras in maintaining steady legal speeds over extended distances, rather than encouraging heavy braking on approach to specific camera locations and subsequent acceleration back to illegal speeds. Italy 
In Italian Highways has developed a monitoring system named Tutor covering more than 2500 km (2012). The Tutor system is also able to intercept cars while changing lanes. The Tutor or Safety Tutor is a joint project between the motorway management company - Autostrade per l'Italia - and the State Police. Over time it has been replaced by other versions for example the SICVe-PM where PM stands for PlateMatching and by the SICVe Vergilius. In addition to this average speed monitoring system, there are others Celeritas and T-Expeed v.2. Netherlands 
Average speed cameras (trajectcontrole) are in place in the Netherlands since 2002. As of July 2009, 12 cameras were operational, mostly in the west of the country and along the A12. Some of these are divided in several ""sections"" to allow for cars leaving and entering the motorway.
A first experimental system was tested on a short stretch of the A2 in 1997 and was deemed a big success by the police, reducing overspeeding to 0.66%, compared to 5 to 6% when regular speed cameras were used at the same location. The first permanent average speed cameras were installed on the A13 in 2002, shortly after the speed limit was reduced to 80 km/h to limit noise and air pollution in the area. In 2007, average speed cameras resulted in 1.7 million fines for overspeeding out of a total of 9.7 millions. According to the Dutch Attorney General, the average number of violation of the speed limits on motorway sections equipped with average speed cameras is between 1 and 2%, compared to 10 to 15% elsewhere. United Kingdom One of the most notable stretches of average speed cameras in the UK is found on the A77 road in Scotland, with 32 miles (51 km) being monitored between Kilmarnock and Girvan. In 2006 it was confirmed that speeding tickets could potentially be avoided from the 'SPECS' cameras by changing lanes and the RAC Foundation feared that people may play ""Russian Roulette"" changing from one lane to another to lessen their odds of being caught; however, in 2007 the system was upgraded for multi-lane use and in 2008 the manufacturer described the ""myth"" as ""categorically untrue"". There exists evidence that implementation of systems such as SPECS has a considerable effect on the volume of drivers travelling at excessive speeds; on the stretch of road mentioned above (A77 Between Glasgow and Ayr) there has been noted a ""huge drop"" in speeding violations since the introduction of a SPECS system.Recent innovations have contributed to the adoption of ANPR for perimeter security and access control applications at government facilities. Within the US, ""homeland security"" efforts to protect against alleged ""acts of terrorism"" have resulted in adoption of ANPR for sensitive facilities such as embassies, schools, airports, maritime ports, military and federal buildings, law enforcement and government facilities, and transportation centers. ANPR is marketed as able to be implemented through networks of IP based surveillance cameras that perform ""double duty"" alongside facial recognition, object tracking, and recording systems for the purpose of monitoring suspicious or anomalous behavior, improving access control, and matching against watch lists. ANPR systems are most commonly installed at points of significant sensitivity, ingress or egress. Major US agencies such as the Department of Homeland Security, the Department of Justice, the Department of Transportation and the Department of Defense have purchased ANPR for perimeter security applications. Large networks of ANPR systems are being installed by cities such as Boston, London and New York City to provide citywide protection against acts of terrorism, and to provide support for public gatherings and public spaces.The Center For Evidence-Based Crime Policy in George Mason University identifies the following randomized controlled trials of automatic number-plate recognition technology as very rigorous.In addition to government facilities, many private sector industries with facility security concerns are beginning to implement ANPR solutions. Examples include casinos, hospitals, museums, parking facilities, and resorts. In the US, private facilities typically cannot access government or police watch lists, but may develop and match against their own databases for customers, VIPs, critical personnel or ""banned person"" lists. In addition to providing perimeter security, private ANPR has service applications for valet / recognized customer and VIP recognition, logistics and key personnel tracking, sales and advertising, parking management, and logistics (vendor and support vehicle tracking).Many cities and districts have developed traffic control systems to help monitor the movement and flow of vehicles around the road network. This had typically involved looking at historical data, estimates, observations and statistics, such as:Car park usage
Pedestrian crossing usage
Number of vehicles along a road
Areas of low and high congestion
Frequency, location and cause of road worksCCTV cameras can be used to help traffic control centres by giving them live data, allowing for traffic management decisions to be made in real-time. By using ANPR on this footage it is possible to monitor the travel of individual vehicles, automatically providing information about the speed and flow of various routes.  These details can highlight problem areas as and when they occur and help the centre to make informed incident management decisions.
Some counties of the United Kingdom have worked with Siemens Traffic to develop traffic monitoring systems for their own control centres and for the public. Projects such as Hampshire County Council's ROMANSE provide an interactive and real-time website showing details about traffic in the city. The site shows information about car parks, ongoing road works, special events and footage taken from CCTV cameras. ANPR systems can be used to provide average point-to-point journey times along particular routes, which can be displayed on a variable-message sign(VMS) giving drivers the ability to plan their route. ROMANSE also allows travellers to see the current situation using a mobile device with an Internet connection (such as WAP, GPRS or 3G), allowing them to view mobile device CCTV images within the Hampshire road network.
The UK company Trafficmaster has used ANPR since 1998 to estimate average traffic speeds on non-motorway roads without the results being skewed by local fluctuations caused by traffic lights and similar. The company now operates a network of over 4000 ANPR cameras, but claims that only the four most central digits are identified, and no numberplate data is retained.IEEE Intelligent Transportation Systems Society published some papers on the plate number recognition technologies and applications. Toll roads Ontario's 407 ETR highway uses a combination of ANPR and radio transponders to toll vehicles entering and exiting the road. Radio antennas are located at each junction and detect the transponders, logging the unique identity of each vehicle in much the same way as the ANPR system does. Without ANPR as a second system it would not be possible to monitor all the traffic. Drivers who opt to rent a transponder for C$2.55 per month are not charged the ""Video Toll Charge"" of C$3.60 for using the road, with heavy vehicles (those with a gross weight of over 5,000 kg) being required to use one. Using either system, users of the highway are notified of the usage charges by post.
There are numerous other electronic toll collection networks which use this combination of Radio frequency identification and ANPR. These include:The Golden Gate Bridge in San Francisco, California, which began using an all-electronic tolling system combining Fastrak and ANPR on March 27, 2013
NC Quick Pass for the Interstate 540 (North Carolina) Triangle Expressway in Wake County, North Carolina
Bridge Pass for the Saint John Harbour Bridge in Saint John, New Brunswick
Quickpass at the Golden Ears Bridge, crossing the Fraser River between Langley and Maple Ridge
e-TAG, Australia
FasTrak in California, United States
Highway 6 in Israel
Tunnels in Hong Kong
Autopista Central  in Santiago, Chile (site in Spanish)
E-ZPass in New York, New Jersey, Pennsylvania, Massachusetts (as Fast Lane until 2012), Virginia (formerly Smart Tag), and other states. Maryland Route 200 uses a combination of E-ZPass and ANPR.
TollTag in North Texas and EZ-Tag in Houston, Texas
I-Pass in Illinois
Pikepass in Oklahoma
Peach Pass I-85 Atlanta, Georgia (Gwinnett County).
OGS (Otomatik Geçiş Sistemi) used at Bosphorus Bridge, Fatih Sultan Mehmet Bridge, and Trans European Motorway entry points in İstanbul, Turkey
M50 Westlink Toll in Dublin, Ireland
Hi-pass in South Korea
Northern Gateway, SH 1, Auckland, New Zealand
Evergreen Point Floating Bridge, Seattle, and Washington State Route 167 HOT-lanes in western Washington
ETC in Taiwan
SunPass In Florida Portugal 
Portuguese roads have old highways with toll stations where drivers can pay with cards and also lanes where there are electronic collection systems. However most new highways only have the option of electronic toll collection system.
The electronic toll collection system comprises three different structures:ANPR which works with infrared cameras and reads license plates from every vehicle
Lasers for volumetric measurement of the vehicle to confirm whether it is a regular car or an SUV or truck, as charges differ according to the type of vehicle
RFID-like to read on-board smart tags.When the smart tag is installed in the vehicle, the car is quickly identified and owner's bank account is automatically deducted. This process is realized at any speed up to over 250 km per hour.
If the car does not have the smart tag, the driver is required to go to a pay station to pay the tolls between 3rd and 5th day after with a surplus charge. If he fails to do so, the owner is sent a letter home with a heavy fine. If this is not paid, it increases five-fold and after that, the car is inserted into a police database for vehicle impounding.
This system is also used in some limited access areas of main cities to allow only entry from pre-registered residents. It is planned to be implemented both in more roads and in city entrance toll collection/access restriction. The efficacy of the system is considered to be so high that it is almost impossible for the driver to complain. London congestion charge The London congestion charge is an example of a system that charges motorists entering a payment area. Transport for London (TfL) uses ANPR systems and charges motorists a daily fee of £11.50 if they enter, leave or move around within the congestion charge zone between 7 a.m. and 6:00 p.m., Monday to Friday. A reduced fee of £10.50 is paid by vehicle owners who sign up for the automatic deduction scheme. Fines for traveling within the zone without paying the charge are £65 per infraction if paid before the deadline, doubling to £130 per infraction thereafter.
There are currently 1,500 cameras which use automatic number plate recognition (ANPR) technology. There are also a number of mobile camera units which may be deployed anywhere in the zone.
It is estimated that around 98% of vehicles moving within the zone are caught on camera. The video streams are transmitted to a data centre located in central London where the ANPR software deduces the registration plate of the vehicle. A second data centre provides a backup location for image data.
Both front and back number plates are being captured, on vehicles going both in and out – this gives up to four chances to capture the number plates of a vehicle entering and exiting the zone. This list is then compared with a list of cars whose owners/operators have paid to enter the zone – those that have not paid are fined. The registered owner of such a vehicle is looked up in a database provided by the DVLA. South Africa 
In Johannesburg, South Africa, ANPR is used for the etoll fee collection. Owners of cars driving into or out of the inner city must pay a charge. The number of tolls passed depends on the distance travelled on the particular freeway. Some of the freeways with ANPR are the N12, N3, N1 etc. Sweden 
In Stockholm, Sweden, ANPR is used for the Stockholm congestion tax, owners of cars driving into or out of the inner city must pay a charge, depending on the time of the day. From 2013, also for the Gothenburg congestion tax, which also includes vehicles passing the city on the main highways.Several UK companies and agencies use ANPR systems. These include Vehicle and Operator Services Agency (VOSA),  Driver and Vehicle Licensing Agency (DVLA) and Transport for London.ANPR systems may also be used for/by:Section control, to measure average vehicle speed over longer distances
Border crossings
Automobile repossessions
Petrol stations to log when a motorist drives away without paying for their fuel
A marketing tool to log patterns of use
Targeted advertising, a-la ""Minority Report""-style billboards
Traffic management systems, which determine traffic flow using the time it takes vehicles to pass two ANPR sites
Analyses of travel behaviour (route choice, origin-destination etc.) for transport planning purposes
Drive-through customer recognition, to automatically recognize customers based on their license plate and offer them the items they ordered the last time they used the service
To assist visitor management systems in recognizing guest vehicles
Police and auxiliary police
Car parking companies
To raise or lower automatic bollards
Hotels
Enforcing Move over laws for emergency vehicles
Automated emissions testingVehicle owners have used a variety of techniques in an attempt to evade ANPR systems and road-rule enforcement cameras in general. One method increases the reflective properties of the lettering and makes it more likely that the system will be unable to locate the plate or produce a high enough level of contrast to be able to read it. This is typically done by using a plate cover or a spray, though claims regarding the effectiveness of the latter are disputed. In most jurisdictions, the covers are illegal and covered under existing laws, while in most countries there is no law to disallow the use of the sprays. Other users have attempted to smear their license plate with dirt or utilize covers to mask the plate.
Novelty frames around Texas license plates were made illegal in Texas on 1 September 2003 by Texas Senate Bill 439 because they caused problems with ANPR devices. That law made it a Class C misdemeanor (punishable by a fine of up to US$200), or Class B (punishable by a fine of up to US$2,000 and 180 days in jail) if it can be proven that the owner did it to deliberately obscure their plates. The law was later clarified in 2007 to allow novelty frames.
If an ANPR system cannot read the plate, it can flag the image for attention, with the human operators looking to see if they are able to identify the alphanumerics.
In 2013 researchers at Sunflex Zone Ltd created a privacy license plate frame that uses near infrared light to make the license plate unreadable to license plate recognition systems.The introduction of ANPR systems has led to fears of misidentification and the furthering of 1984-style surveillance. In the United States, some such as Gregg Easterbrook oppose what they call ""machines that issue speeding tickets and red-light tickets"" as the beginning of a slippery slope towards an automated justice system:""A machine classifies a person as an offender, and you can't confront your accuser because there is no accuser... can it be wise to establish a principle that when a machine says you did something illegal, you are presumed guilty?""Similar criticisms have been raised in other countries. Easterbrook also argues that this technology is employed to maximize revenue for the state, rather than to promote safety.
The electronic surveillance system produces tickets which in the US are often in excess of $100, and are virtually impossible for a citizen to contest in court without the help of an attorney. The revenues generated by these machines are shared generously with the private corporation that builds and operates them, creating a strong incentive to tweak the system to generate as many tickets as possible.
Older systems had been notably unreliable; in the UK this has been known to lead to charges being made incorrectly with the vehicle owner having to pay £10 in order to be issued with proof (or not) of the offense. Improvements in technology have drastically decreased error rates, but false accusations are still frequent enough to be a problem.
Perhaps the best known incident involving the abuse of an ANPR database in North America is the case of Edmonton Sun reporter Kerry Diotte in 2004. Diotte wrote an article critical of Edmonton police use of traffic cameras for revenue enhancement, and in retaliation was added to an ANPR database of ""high-risk drivers"" in an attempt to monitor his habits and create an opportunity to arrest him. The police chief and several officers were fired as a result, and The Office of the Privacy Commissioner of Canada expressed public concern over the ""growing police use of technology to spy on motorists.""Other concerns include the storage of information that could be used to identify people and store details about their driving habits and daily life, contravening the Data Protection Act along with similar legislation (see personally identifiable information). The laws in the UK are strict for any system that uses CCTV footage and can identify individuals.Also of concern is the safety of the data once it is mined, following the discovery of police surveillance records lost in a gutter.There is also a case in the UK for saying that use of ANPR cameras is unlawful under the Regulation of Investigatory Powers Act 2000. The breach exists, some say, in the fact that ANPR is used to monitor the activities of law-abiding citizens and treats everyone like the suspected criminals intended to be surveyed under the Act. The police themselves have been known to refer to the system of ANPR as a ""24/7 traffic movement database"" which is a diversion from its intended purpose of identifying vehicles involved in criminal activities. The opposing viewpoint is that where the plates have been cloned, a 'read' of an innocent motorist's vehicle will allow the elimination of that vehicle from an investigation by visual examination of the images stored. Likewise, stolen vehicles are read by ANPR systems between the time of theft and report to the Police, assisting in the investigation.
The Associated Press reported in August 2011 that New York Police Department cars and license plate tracking equipment purchased with federal HIDTA (High Intensity Drug Trafficking Area) funds were used to spy on Muslims at mosques, and to track the license plate numbers of worshipers.
Police in unmarked cars outfitted with electronic license plate readers would drive down the street and automatically catalog the plates of everyone parked near the mosque, amassing a covert database that would be distributed among officers and used to profile Muslims in public.In 2013 the American Civil Liberties Union (ACLU) released 26,000 pages of data about ANPR systems obtained from local, state, and federal agencies through freedom of information laws.  ""The documents paint a startling picture of a technology deployed with too few rules that is becoming a tool for mass routine location tracking and surveillance"" wrote the ACLU.  The ACLU reported that in many locations the devices were being used to store location information on vehicles which were not suspected of any particular offense.  ""Private companies are also using license plate readers and sharing the information they collect with police with little or no oversight or privacy protections. A lack of regulation means that policies governing how long our location data is kept vary widely,"" the ACLU said. In 2012 the ACLU filed suit against the Department of Homeland Security, which funds many local and state ANPR programs through grants, after the agency failed to provide access to records the ACLU had requested under the Freedom of Information Act about the programs.In mid-August 2015, in Boston, it was discovered that the license plate records for a million people was online and unprotected.In April 2020, The Register UK with the help of security researchers discovered nine million ANPR logs left wide-open on the internet. The 3M Sheffield Council system had been online and unprotected since 2013-2014 In the United States, inaccurate results have led to unnecessary stops of innocent people. The most notable case involved a Black family in Aurora, Colorado with four children aged between 6 and 17 being held at gunpoint, and the children placed in handcuffs.Many ANPR systems claim accuracy when trained to match plates from a single jurisdiction or region, but can fail when trying to recognize plates from other jurisdictions due to variations in format, font, color, layout, and other plate features. Some jurisdictions offer vanity or affinity plates (particularly in the US), which can create many variations within a single jurisdiction.From time to time, US states will make significant changes in their license plate protocol that will affect OCR accuracy. They may add a character or add a new license plate design. ALPR systems must adapt to these changes quickly in order to be effective. Another challenge with ALPR systems is that some states have the same license plate protocol. For example, more than one state uses the standard three letters followed by four numbers. So each time the ALPR systems alarms, it is the user's responsibility to make sure that the plate which caused the alarm matches the state associated with the license plate listed on the in-car computer. For maximum effectiveness, an ANPR system should be able to recognize plates from any jurisdiction, and the jurisdiction to which they are associated, but these many variables make such tasks difficult.
Currently at least one US ANPR provider (PlateSmart) claims their system has been independently reviewed as able to accurately recognize the US state jurisdiction of license plates, and one European ANPR provider claims their system can differentiate all EU plate jurisdictions.A few ANPR software vendors publish accuracy results based on image benchmarks. These results may vary depending on which images the vendor has chosen to include in their test. In 2017, Sighthound reported a 93.6% accuracy on a private image benchmark. In 2017, OpenALPR reported accuracy rates for their commercial software in the range of 95-98% on a public image benchmark. April 2018 research from Brazil's Federal University of Paraná and Federal University of Minas Gerais obtained a recognition rate of 93.0% for OpenALPR and 89.8% for Sighthound, running both on the SSIG dataset; and a rate of 93.5% for a system of their own design based on the YOLO object detector, also using the SSIG dataset. Testing a ""more realistic scenario"" involving both plate and reader moving, the researchers obtained rates of less than 70% for the two commercial systems and 78.3% for their own.AI effect
Applications of artificial intelligence
Facial recognition system
Road policing unit
Vehicle location dataListsList of emerging technologies
Outline of artificial intelligence"
"PRODIGAL (computer system)","PRODIGAL (proactive discovery of insider threats using graph analysis and learning) is a computer system for predicting anomalous behavior among humans, by data mining network traffic such as emails, text messages and server log entries. It is part of DARPA's Anomaly Detection at Multiple Scales (ADAMS) project. The initial schedule is for two years and the budget $9 million.It uses graph theory, machine learning, statistical anomaly detection, and high-performance computing to scan larger sets of data more quickly than in past systems. The amount of data analyzed is in the range of terabytes per day. The targets of the analysis are employees within the government or defense contracting organizations; specific examples of behavior the system is intended to detect include the actions of Nidal Malik Hasan and WikiLeaks source Chelsea Manning. Commercial applications may include finance. The results of the analysis, the five most serious threats per day, go to agents, analysts, and operators working in counterintelligence.Georgia Institute of Technology College of Computing
Georgia Tech Research Institute
Defense Advanced Research Projects Agency
Army Research Office
Science Applications International Corporation
Oregon State University
University of Massachusetts Amherst
Carnegie Mellon UniversityCyber Insider Threat
Einstein (US-CERT program)
Threat (computer)
Intrusion detection
ECHELON, Thinthread, Trailblazer, Turbulence (NSA programs)
Fusion center, Investigative Data Warehouse (FBI)"
"Text mining","Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources."" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).
Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.
A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
The document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of ""text mining"" in 2004 to describe ""text analytics"". The latter term is now used more frequently in business settings while ""text mining"" is used in some of the earliest application areas, dating to the 1980s, notably life-sciences research and government intelligence.
The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.Subtasks—components of a larger text-analytics effort—typically include:Dimensionality reduction is important technique for pre-processing data. Technique is used to identify the root word for actual words and reduce the size of the text data.
Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.
Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.
Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on.
Disambiguation—the use of contextual clues—may be required to decide where, for instance, ""Ford"" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.
Recognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.
Document clustering: identification of sets of similar text documents.
Coreference: identification of noun phrases and other terms that refer to the same object.
Relationship, fact, and event Extraction: identification of associations among entities and other information in text
Sentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion. Text analytics techniques are helpful in analyzing sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.
Quantitative text analysis is a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.
Pre-processing usually involves tasks such as tokenization, filtering and stemming.Text mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities. Legal professionals may use text mining for e-discovery, for example. Governments and military groups use text mining for national security and intelligence purposes. Scientific researchers incorporate text mining approaches into efforts to organize large sets of text data (i.e., addressing the problem of unstructured data), to determine ideas communicated through text (e.g., sentiment analysis in social media) and to support scientific discovery in fields such as the life sciences and bioinformatics. In business, applications are used to support competitive intelligence and automated ad placement, among numerous other activities.Many text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes. It is also involved in the study of text encryption/decryption.A range of text mining applications in the biomedical literature has been described, including computational approaches to assist with studies in protein docking, protein interactions, and protein-disease associations. In addition, with large patient textual datasets in the clinical field, datasets of demographic information in population studies and adverse event reports, text mining can facilitate clinical studies and precision medicine. Text mining algorithms can facilitate the stratification and indexing of specific clinical events in large patient textual datasets of symptoms, side effects, and comorbidities from electronic health records, event reports, and reports from specific diagnostic tests. One online text mining application in the biomedical literature is PubGene, a publicly accessible search engine that combines biomedical text mining with network visualization. GoPubMed is a knowledge-based search engine for biomedical texts. Text mining techniques also enable us to extract unknown knowledge from unstructured documents in the clinical domainText mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities. For study purposes, Weka software is one of the most popular options in the scientific world, acting as an excellent entry point for beginners. For Python programmers, there is an excellent toolkit called NLTK for more general purposes. For more advanced programmers, there's also the Gensim library, which focuses on word embedding-based text representations.Text mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site ""stickiness"" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.Text analytics is being used in business, particularly, in marketing, such as in customer relationship management. Coussement and Van den Poel (2008) apply it to improve predictive analytics models for customer churn (customer attrition). Text mining is also being applied in stock returns prediction.Sentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.
Such an analysis may need a labeled data set or labeling of the affectivity of words.
Resources for affectivity of words and concepts have been made for WordNet and ConceptNet, respectively.
Text has been used to detect emotions in the related area of affective computing. Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.The issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within the written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within the text without removing publisher barriers to public access.
Academic institutions have also become involved in the text mining initiative:The National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester in close collaboration with the Tsujii Lab, University of Tokyo. NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK research councils (EPSRC & BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.
In the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.
The Text Analysis Portal for Research (TAPoR), currently housed at the University of Alberta, is a scholarly project to catalogue text analysis applications and create a gateway for researchers new to the practice. Methods for scientific literature mining 
Computational methods have been developed to assist with information retrieval from scientific literature. Published approaches include methods for searching, determining novelty, and clarifying homonyms among technical reports.The automatic analysis of vast textual corpora has created the possibility for scholars to analyze
millions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been parsing, machine translation, topic categorization, and machine learning.The automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data.  The resulting networks, which can contain thousands of nodes, are then analyzed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. This automates the approach introduced by quantitative narrative analysis, whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.Content analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a ""big data"" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al. showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analyzing Twitter content was demonstrated as well.Text mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is illegal. In the UK in 2014, on the recommendation of the Hargreaves review, the government amended copyright law to allow text mining as a limitation and exception. It was  the second country in the world to do so, following Japan, which introduced a mining-specific exception in 2009. However, owing to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licenses for Europe. The fact that the focus on the solution to this legal issue was licenses, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.US copyright law, and in particular its fair use provisions, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one such use being text and data mining.Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events.  For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence.  In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.Increasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.
The challenge of exploiting the large proportion of enterprise information that originates in ""unstructured"" form has been recognized for decades. It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:""...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.""Yet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in ""unstructured"" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:
For almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.Hearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later."
"Zapaday","Zapaday is a global news calendar. The website publishes upcoming news headlines per day and per topic as a resource for journalists, bloggers, political analysts, marketers, event organisers, public relation professionals, scientists and travellers. Zapaday uses both bots and human editors to monitor over 4,000 news sites and calendars for future news stories, publishing its findings as news events on categorized calendars.
Users can create and publish their own events and calendars, re-using events and calendars of others for personal use. On March 6, 2014, Zapaday launched a new subscription service with paid premium news calendars. The company also announced a new marketplace where journalists can syndicate curated news calendars as premium content. The new model invites journalists and content creators to publish premium calendars and receive 50 per cent of earnings, while Zapaday will receive 20 per cent for hosting and handling. The remaining 30 per cent go to the seller, who can be either the content creator themselves, Zapaday, or one of the news agencies or other resellers that offer a white-label version of Zapaday to their clients.
Calendar events from Zapaday can be exported to a user's Outlook, Google Calendar, or a mobile phone at any time.Zapaday won an award as most promising start-up company across Europe at Tech Media Europe 2011 and was a 2012 Accenture Innovation Awards finalist.In December 2013, Zapaday, together with UK's GRNlive, the Foreign Correspondents Network, launched a new global reporting service where each future news event on Zapaday is accompanied with GRNlive journalists available in the region to cover the story on the ground.On 23 January 2013, Zapaday, together with Dutch press agency Algemeen Nederlands Persbureau (ANP), launched the renewed ‘ANP Agenda’ based on the Zapaday platform. The ANP Agenda includes planned domestic and sports news events, curated by ANP editors, and foreign and economic events curated by both ANP and Zapaday editors.
Nearly 1.000 users from ANP, including journalists, broadcasters and communication professionals, now use the platform to spot upcoming news events and plan ahead.Reuters
Algemeen Nederlands Persbureau
Agence France-Presse
Associated Press
Recorded Future"
